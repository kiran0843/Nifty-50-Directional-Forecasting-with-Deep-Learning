{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kiran0843/Nifty-50-Directional-Forecasting-with-Deep-Learning/blob/main/Copy_of_niftylstm_cnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_d6tCst1HYI2",
        "outputId": "b764b394-8492-42e0-b6cb-60c2fe57aab7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement pandas_ta==0.3.14b0 (from versions: 0.4.67b0, 0.4.71b0)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for pandas_ta==0.3.14b0\u001b[0m\u001b[31m\n",
            "\u001b[0müéØ NIFTY 50 DIRECTIONAL FORECASTING - ROBUST ALIGNMENT\n",
            "======================================================================\n",
            "üìÖ Setting up 7-year date range to TODAY...\n",
            "Data range: 2018-12-03 to 2025-12-03 (TODAY)\n",
            "Current time: 2025-12-03 03:41:03 IST\n",
            "\n",
            "üìà Downloading data for 9 assets...\n",
            "  Downloading ^NSEI...\n",
            "    ‚úÖ 1727 samples, last data: 2025-12-02\n",
            "  Downloading ^NSEBANK...\n",
            "    ‚úÖ 1720 samples, last data: 2025-12-02\n",
            "  Downloading ^NSMIDCP...\n",
            "    ‚úÖ 1719 samples, last data: 2025-12-02\n",
            "  Downloading ^BSESN...\n",
            "    ‚úÖ 1723 samples, last data: 2025-12-02\n",
            "  Downloading ^DJI...\n",
            "    ‚úÖ 1759 samples, last data: 2025-12-02\n",
            "  Downloading ^GSPC...\n",
            "    ‚úÖ 1759 samples, last data: 2025-12-02\n",
            "  Downloading ^IXIC...\n",
            "    ‚úÖ 1759 samples, last data: 2025-12-02\n",
            "  Downloading USDINR=X...\n",
            "    ‚úÖ 1823 samples, last data: 2025-12-02\n",
            "  Downloading CL=F...\n",
            "    ‚úÖ 1757 samples, last data: 2025-12-02\n",
            "Successfully downloaded 9 assets\n",
            "\n",
            "üîó Creating extended date index to TODAY...\n",
            "Extended date range: 1828 business days to TODAY\n",
            "Range: 2018-12-03 to 2025-12-03\n",
            "\n",
            "üìÖ Robustly aligning all assets to TODAY...\n",
            "  Processing ^NSEI...\n",
            "    Original: 1727, Filled: 1828, Coverage: 100.00%\n",
            "    Last actual data: 2025-12-02 (1 days to today)\n",
            "    ‚úÖ Kept (extended to TODAY)\n",
            "  Processing ^NSEBANK...\n",
            "    Original: 1720, Filled: 1828, Coverage: 100.00%\n",
            "    Last actual data: 2025-12-02 (1 days to today)\n",
            "    ‚úÖ Kept (extended to TODAY)\n",
            "  Processing ^NSMIDCP...\n",
            "    Original: 1719, Filled: 1828, Coverage: 100.00%\n",
            "    Last actual data: 2025-12-02 (1 days to today)\n",
            "    ‚úÖ Kept (extended to TODAY)\n",
            "  Processing ^BSESN...\n",
            "    Original: 1723, Filled: 1828, Coverage: 100.00%\n",
            "    Last actual data: 2025-12-02 (1 days to today)\n",
            "    ‚úÖ Kept (extended to TODAY)\n",
            "  Processing ^DJI...\n",
            "    Original: 1759, Filled: 1828, Coverage: 100.00%\n",
            "    Last actual data: 2025-12-02 (1 days to today)\n",
            "    ‚úÖ Kept (extended to TODAY)\n",
            "  Processing ^GSPC...\n",
            "    Original: 1759, Filled: 1828, Coverage: 100.00%\n",
            "    Last actual data: 2025-12-02 (1 days to today)\n",
            "    ‚úÖ Kept (extended to TODAY)\n",
            "  Processing ^IXIC...\n",
            "    Original: 1759, Filled: 1828, Coverage: 100.00%\n",
            "    Last actual data: 2025-12-02 (1 days to today)\n",
            "    ‚úÖ Kept (extended to TODAY)\n",
            "  Processing USDINR=X...\n",
            "    Original: 1823, Filled: 1828, Coverage: 100.00%\n",
            "    Last actual data: 2025-12-02 (1 days to today)\n",
            "    ‚úÖ Kept (extended to TODAY)\n",
            "  Processing CL=F...\n",
            "    Original: 1757, Filled: 1828, Coverage: 100.00%\n",
            "    Last actual data: 2025-12-02 (1 days to today)\n",
            "    ‚úÖ Kept (extended to TODAY)\n",
            "\n",
            "‚úÖ Successfully aligned 9 assets\n",
            "\n",
            "üîÑ Creating multi-asset DataFrame extended to TODAY...\n",
            "‚úÖ Multi-asset DataFrame created (EXTENDED TO TODAY):\n",
            "   Total samples: 16,452\n",
            "   Assets: 9\n",
            "   Date range: 2018-12-03 to 2025-12-03\n",
            "   ‚úÖ SUCCESS: Data extends to TODAY (2025-12-03)\n",
            "   Primary (Nifty 50): 1,828 samples\n",
            "   Latest Nifty data: 2025-12-03, Close: 26032.20\n",
            "   üìã Note: Latest data appears forward-filled from 2025-12-02\n",
            "\n",
            "üéØ READY FOR NEXT-DAY FORECASTING!\n",
            "‚úÖ Data collection to TODAY complete - pipeline always up-to-date!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "\n",
        "try:\n",
        "    import pandas_ta\n",
        "except ImportError:\n",
        "    !pip install pandas_ta==0.3.14b0\n",
        "\n",
        "print(\"üéØ NIFTY 50 DIRECTIONAL FORECASTING - ROBUST ALIGNMENT\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "\n",
        "print(\"üìÖ Setting up 7-year date range to TODAY...\")\n",
        "today = pd.Timestamp.now().normalize()\n",
        "start_date = today - pd.DateOffset(years=7)\n",
        "\n",
        "print(f\"Data range: {start_date.strftime('%Y-%m-%d')} to {today.strftime('%Y-%m-%d')} (TODAY)\")\n",
        "print(f\"Current time: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S IST')}\")\n",
        "\n",
        "\n",
        "primary_symbol = \"^NSEI\"  # Nifty 50\n",
        "correlated_symbols = [\n",
        "    \"^NSEBANK\",   # Bank Nifty\n",
        "    \"^NSMIDCP\",   # Nifty Midcap\n",
        "    \"^BSESN\",     # BSE Sensex\n",
        "    \"^DJI\",       # Dow Jones\n",
        "    \"^GSPC\",      # S&P 500\n",
        "    \"^IXIC\",      # NASDAQ\n",
        "    \"USDINR=X\",   # USD/INR\n",
        "    \"CL=F\"        # Crude Oil WTI\n",
        "]\n",
        "\n",
        "all_symbols = [primary_symbol] + correlated_symbols\n",
        "\n",
        "\n",
        "print(f\"\\nüìà Downloading data for {len(all_symbols)} assets...\")\n",
        "\n",
        "raw_assets_data = {}\n",
        "for symbol in all_symbols:\n",
        "    try:\n",
        "        print(f\"  Downloading {symbol}...\")\n",
        "        asset_df = yf.download(\n",
        "            symbol,\n",
        "            start=start_date.strftime('%Y-%m-%d'),\n",
        "            end=today.strftime('%Y-%m-%d'),\n",
        "            progress=False\n",
        "        )\n",
        "\n",
        "        if not asset_df.empty:\n",
        "            # Handle MultiIndex columns if present\n",
        "            if isinstance(asset_df.columns, pd.MultiIndex):\n",
        "                asset_df.columns = asset_df.columns.droplevel(1)\n",
        "\n",
        "            # Keep only OHLCV columns if available\n",
        "            available_cols = [col for col in ['Open', 'High', 'Low', 'Close', 'Volume']\n",
        "                             if col in asset_df.columns]\n",
        "            if available_cols:\n",
        "                asset_df = asset_df[available_cols]\n",
        "                raw_assets_data[symbol] = asset_df\n",
        "                last_date = asset_df.index.max().strftime('%Y-%m-%d')\n",
        "                print(f\"    ‚úÖ {len(asset_df)} samples, last data: {last_date}\")\n",
        "            else:\n",
        "                print(f\"    ‚ùå No OHLCV columns found\")\n",
        "        else:\n",
        "            print(f\"    ‚ùå No data downloaded\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"    ‚ùå Failed: {str(e)}\")\n",
        "\n",
        "print(f\"Successfully downloaded {len(raw_assets_data)} assets\")\n",
        "\n",
        "# ===== STEP 4: CREATE EXTENDED DATE INDEX TO TODAY =====\n",
        "print(f\"\\nüîó Creating extended date index to TODAY...\")\n",
        "\n",
        "# Get earliest date from any asset\n",
        "earliest_date = min(df.index.min() for df in raw_assets_data.values())\n",
        "\n",
        "# Create business day range from earliest date to TODAY\n",
        "extended_dates = pd.date_range(start=earliest_date, end=today, freq='B')\n",
        "print(f\"Extended date range: {len(extended_dates)} business days to TODAY\")\n",
        "print(f\"Range: {earliest_date.strftime('%Y-%m-%d')} to {today.strftime('%Y-%m-%d')}\")\n",
        "\n",
        "# ===== STEP 5: ROBUST ALIGNMENT FUNCTION =====\n",
        "def robust_align_and_fill(asset_df, target_index, max_forward_fill=10):\n",
        "    \"\"\"\n",
        "    Robustly align asset data to target index and forward fill\n",
        "    \"\"\"\n",
        "    # Create empty DataFrame with target index\n",
        "    aligned_df = pd.DataFrame(index=target_index, columns=asset_df.columns, dtype=float)\n",
        "\n",
        "    # Use reindex to safely align asset data to target index\n",
        "    asset_reindexed = asset_df.reindex(target_index)\n",
        "\n",
        "    # Fill the aligned DataFrame (this avoids KeyError)\n",
        "    for col in asset_df.columns:\n",
        "        aligned_df[col] = asset_reindexed[col]\n",
        "\n",
        "    # Apply forward fill with limit using pandas method\n",
        "    for col in aligned_df.columns:\n",
        "        aligned_df[col] = aligned_df[col].ffill(limit=max_forward_fill)\n",
        "\n",
        "    return aligned_df\n",
        "\n",
        "# ===== STEP 6: ROBUSTLY ALIGN ALL ASSETS TO TODAY'S DATE =====\n",
        "print(f\"\\nüìÖ Robustly aligning all assets to TODAY...\")\n",
        "\n",
        "aligned_assets = {}\n",
        "for symbol, asset_df in raw_assets_data.items():\n",
        "    print(f\"  Processing {symbol}...\")\n",
        "\n",
        "    # Use robust alignment function\n",
        "    aligned_filled = robust_align_and_fill(asset_df, extended_dates, max_forward_fill=10)\n",
        "\n",
        "    # Calculate data quality metrics\n",
        "    original_samples = len(asset_df)\n",
        "    filled_samples = int(aligned_filled['Close'].count())\n",
        "    fill_ratio = float(filled_samples) / float(len(aligned_filled))\n",
        "\n",
        "    # Get last actual data date vs today\n",
        "    last_actual = asset_df.index.max()\n",
        "    days_to_fill = (today - last_actual).days if today > last_actual else 0\n",
        "\n",
        "    print(f\"    Original: {original_samples}, Filled: {filled_samples}, Coverage: {fill_ratio:.2%}\")\n",
        "    print(f\"    Last actual data: {last_actual.strftime('%Y-%m-%d')} ({days_to_fill} days to today)\")\n",
        "\n",
        "    if fill_ratio > 0.3:  # Lower threshold to keep more assets for forecasting\n",
        "        aligned_assets[symbol] = aligned_filled\n",
        "        print(f\"    ‚úÖ Kept (extended to TODAY)\")\n",
        "    else:\n",
        "        print(f\"    ‚ùå Dropped (low coverage)\")\n",
        "\n",
        "print(f\"\\n‚úÖ Successfully aligned {len(aligned_assets)} assets\")\n",
        "\n",
        "# ===== STEP 7: CREATE MULTI-ASSET DATAFRAME TO TODAY =====\n",
        "print(f\"\\nüîÑ Creating multi-asset DataFrame extended to TODAY...\")\n",
        "\n",
        "# Stack all assets into single DataFrame\n",
        "asset_data_list = []\n",
        "for symbol, df in aligned_assets.items():\n",
        "    df_copy = df.copy()\n",
        "    df_copy['Symbol'] = symbol\n",
        "    df_copy = df_copy.reset_index()\n",
        "    df_copy = df_copy.rename(columns={'index': 'Date'})\n",
        "    asset_data_list.append(df_copy)\n",
        "\n",
        "# Combine all assets\n",
        "multi_asset_df = pd.concat(asset_data_list, ignore_index=True)\n",
        "multi_asset_df = multi_asset_df.dropna(subset=['Close'])\n",
        "\n",
        "print(f\"‚úÖ Multi-asset DataFrame created (EXTENDED TO TODAY):\")\n",
        "print(f\"   Total samples: {len(multi_asset_df):,}\")\n",
        "print(f\"   Assets: {multi_asset_df['Symbol'].nunique()}\")\n",
        "print(f\"   Date range: {multi_asset_df['Date'].min().strftime('%Y-%m-%d')} to {multi_asset_df['Date'].max().strftime('%Y-%m-%d')}\")\n",
        "\n",
        "# Check if we actually reached today\n",
        "max_date_in_data = multi_asset_df['Date'].max()\n",
        "if max_date_in_data.date() == today.date():\n",
        "    print(f\"   ‚úÖ SUCCESS: Data extends to TODAY ({today.strftime('%Y-%m-%d')})\")\n",
        "elif max_date_in_data.date() >= (today - pd.Timedelta(days=3)).date():\n",
        "    print(f\"   ‚úÖ RECENT: Data extends to {max_date_in_data.strftime('%Y-%m-%d')} (within 3 days)\")\n",
        "else:\n",
        "    print(f\"   ‚ö†Ô∏è GAP: Latest data is {max_date_in_data.strftime('%Y-%m-%d')}, today is {today.strftime('%Y-%m-%d')}\")\n",
        "\n",
        "# Separate primary asset for main modeling\n",
        "nifty_data = multi_asset_df[multi_asset_df['Symbol'] == primary_symbol].copy()\n",
        "nifty_data = nifty_data.sort_values('Date').reset_index(drop=True)\n",
        "\n",
        "print(f\"   Primary (Nifty 50): {len(nifty_data):,} samples\")\n",
        "\n",
        "# Show the most recent Nifty 50 data to confirm\n",
        "if len(nifty_data) > 0:\n",
        "    latest_nifty = nifty_data.iloc[-1]\n",
        "    print(f\"   Latest Nifty data: {latest_nifty['Date'].strftime('%Y-%m-%d')}, Close: {latest_nifty['Close']:.2f}\")\n",
        "\n",
        "    # Show if we have today's data or forward-filled data\n",
        "    second_latest = nifty_data.iloc[-2] if len(nifty_data) > 1 else None\n",
        "    if second_latest is not None and latest_nifty['Close'] == second_latest['Close']:\n",
        "        print(f\"   üìã Note: Latest data appears forward-filled from {second_latest['Date'].strftime('%Y-%m-%d')}\")\n",
        "    else:\n",
        "        print(f\"   üìã Latest data appears to be actual market data\")\n",
        "\n",
        "print(f\"\\nüéØ READY FOR NEXT-DAY FORECASTING!\")\n",
        "print(f\"‚úÖ Data collection to TODAY complete - pipeline always up-to-date!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9mQMGJyWtDQN",
        "outputId": "234c0aee-3654-4c43-bad0-42d0cc1c13ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "CELL 1B: EXTERNAL DATA + ADVANCED FEATURES V2\n",
            "======================================================================\n",
            "\n",
            "Fetching external data: 2018-08-25 to 2025-12-03\n",
            "\n",
            "Downloading India VIX...\n",
            "  ‚úÖ 1776 rows\n",
            "Downloading USD/INR...\n",
            "  ‚úÖ 1893 rows\n",
            "Downloading Crude Oil...\n",
            "  ‚úÖ 1825 rows\n",
            "Downloading S&P 500...\n",
            "  ‚úÖ 1827 rows\n",
            "Downloading Hang Seng...\n",
            "  ‚úÖ 1789 rows\n",
            "Downloading US 10Y...\n",
            "  ‚úÖ 1827 rows\n",
            "\n",
            "‚úÖ Downloaded 6 external datasets\n",
            "\n",
            "üìä Enhanced dataset: (1828, 13)\n",
            "   New columns: ['INDIAVIX', 'USDINR', 'OIL', 'SP500', 'HSI', 'US10Y']\n",
            "\n",
            "üîß Creating advanced features...\n",
            "\n",
            "  ‚Ä¢ Volatility features...\n",
            "  ‚Ä¢ Volume/microstructure features...\n",
            "  ‚Ä¢ Cross-asset correlations...\n",
            "  ‚Ä¢ Lagged features...\n",
            "  ‚Ä¢ Regime features...\n",
            "  ‚Ä¢ High-performance features...\n",
            "  ‚úÖ All features created\n",
            "\n",
            "======================================================================\n",
            "‚úÖ ENHANCED DATASET READY V2\n",
            "   Original features: 7\n",
            "   Enhanced features: 72\n",
            "   New features added: 65\n",
            "\n",
            "üìä Expected usable features in Cell 8: ~57\n",
            "======================================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import yfinance as yf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"CELL 1B: EXTERNAL DATA + ADVANCED FEATURES V2\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "start_date = pd.to_datetime(nifty_data['Date'].min()) - timedelta(days=100)\n",
        "end_date = pd.to_datetime(nifty_data['Date'].max())\n",
        "\n",
        "print(f\"Fetching external data: {start_date.date()} to {end_date.date()}\\n\")\n",
        "\n",
        "def safe_download(ticker, name, start, end):\n",
        "    try:\n",
        "        print(f\"Downloading {name}...\")\n",
        "        data = yf.download(ticker, start=start, end=end, progress=False)\n",
        "        if len(data) > 0:\n",
        "            print(f\"  ‚úÖ {len(data)} rows\")\n",
        "            return data\n",
        "        else:\n",
        "            print(f\"  ‚ö†Ô∏è No data\")\n",
        "            return None\n",
        "    except Exception as e:\n",
        "        print(f\"  ‚ùå Error: {e}\")\n",
        "        return None\n",
        "\n",
        "external_data = {}\n",
        "\n",
        "indiavix = safe_download(\"^INDIAVIX\", \"India VIX\", start_date, end_date)\n",
        "if indiavix is not None:\n",
        "    external_data['INDIAVIX'] = indiavix[['Close']].rename(columns={'Close': 'INDIAVIX'})\n",
        "\n",
        "usdinr = safe_download(\"USDINR=X\", \"USD/INR\", start_date, end_date)\n",
        "if usdinr is not None:\n",
        "    external_data['USDINR'] = usdinr[['Close']].rename(columns={'Close': 'USDINR'})\n",
        "\n",
        "oil = safe_download(\"CL=F\", \"Crude Oil\", start_date, end_date)\n",
        "if oil is not None:\n",
        "    external_data['OIL'] = oil[['Close']].rename(columns={'Close': 'OIL'})\n",
        "\n",
        "sp500 = safe_download(\"^GSPC\", \"S&P 500\", start_date, end_date)\n",
        "if sp500 is not None:\n",
        "    external_data['SP500'] = sp500[['Close']].rename(columns={'Close': 'SP500'})\n",
        "\n",
        "hsi = safe_download(\"^HSI\", \"Hang Seng\", start_date, end_date)\n",
        "if hsi is not None:\n",
        "    external_data['HSI'] = hsi[['Close']].rename(columns={'Close': 'HSI'})\n",
        "\n",
        "us10y = safe_download(\"^TNX\", \"US 10Y\", start_date, end_date)\n",
        "if us10y is not None:\n",
        "    external_data['US10Y'] = us10y[['Close']].rename(columns={'Close': 'US10Y'})\n",
        "\n",
        "print(f\"\\n‚úÖ Downloaded {len(external_data)} external datasets\\n\")\n",
        "\n",
        "df_enhanced = nifty_data.copy()\n",
        "df_enhanced['Date'] = pd.to_datetime(df_enhanced['Date'])\n",
        "df_enhanced = df_enhanced.sort_values('Date').reset_index(drop=True)\n",
        "\n",
        "for name, data in external_data.items():\n",
        "    data_reset = data.reset_index()\n",
        "    data_reset.columns = ['Date', name]\n",
        "    data_reset['Date'] = pd.to_datetime(data_reset['Date'])\n",
        "\n",
        "    df_enhanced = pd.merge(df_enhanced, data_reset, on='Date', how='left', suffixes=('', '_new'))\n",
        "\n",
        "    if f'{name}_new' in df_enhanced.columns:\n",
        "        df_enhanced[name] = df_enhanced[f'{name}_new']\n",
        "        df_enhanced = df_enhanced.drop(columns=[f'{name}_new'])\n",
        "\n",
        "    if name in df_enhanced.columns:\n",
        "        df_enhanced[name] = df_enhanced[name].fillna(method='ffill').fillna(method='bfill')\n",
        "\n",
        "print(f\"üìä Enhanced dataset: {df_enhanced.shape}\")\n",
        "print(f\"   New columns: {[c for c in df_enhanced.columns if c not in nifty_data.columns]}\\n\")\n",
        "\n",
        "print(\"üîß Creating advanced features...\\n\")\n",
        "\n",
        "def create_advanced_features(df):\n",
        "    df = df.sort_values('Date').copy()\n",
        "\n",
        "    df['Return_1d'] = df['Close'].pct_change()\n",
        "    df['Log_Return'] = np.log(df['Close'] / df['Close'].shift(1))\n",
        "\n",
        "    print(\"  ‚Ä¢ Volatility features...\")\n",
        "    df['RealVol_5d'] = df['Return_1d'].rolling(5).std()\n",
        "    df['RealVol_10d'] = df['Return_1d'].rolling(10).std()\n",
        "    df['RealVol_20d'] = df['Return_1d'].rolling(20).std()\n",
        "    df['Parkinson_Vol'] = np.sqrt(1/(4*np.log(2)) * (np.log(df['High']/df['Low'])**2).rolling(10).mean())\n",
        "\n",
        "    high_low = df['High'] - df['Low']\n",
        "    high_close = np.abs(df['High'] - df['Close'].shift())\n",
        "    low_close = np.abs(df['Low'] - df['Close'].shift())\n",
        "    true_range = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)\n",
        "    df['ATR_14'] = true_range.rolling(14).mean()\n",
        "    df['Vol_Adj_Return'] = df['Return_1d'] / (df['RealVol_20d'] + 1e-6)\n",
        "\n",
        "    print(\"  ‚Ä¢ Volume/microstructure features...\")\n",
        "    df['VWAP'] = (df['High'] + df['Low'] + df['Close']) / 3\n",
        "    df['VWAP_Distance'] = (df['Close'] - df['VWAP']) / (df['VWAP'] + 1e-8)\n",
        "    df['Volume_MA20'] = df['Volume'].rolling(20).mean()\n",
        "    df['Volume_Ratio'] = df['Volume'] / (df['Volume_MA20'] + 1)\n",
        "    df['Volume_Momentum'] = df['Volume'].pct_change(5)\n",
        "    df['PVT'] = ((df['Close'] - df['Close'].shift()) / (df['Close'].shift() + 1e-8) * df['Volume']).cumsum()\n",
        "    df['PVT_MA'] = df['PVT'].rolling(20).mean()\n",
        "\n",
        "    print(\"  ‚Ä¢ Cross-asset correlations...\")\n",
        "    if 'INDIAVIX' in df.columns:\n",
        "        df['VIX_Level'] = df['INDIAVIX']\n",
        "        df['VIX_Change'] = df['INDIAVIX'].pct_change()\n",
        "        df['VIX_MA10'] = df['INDIAVIX'].rolling(10, min_periods=5).mean()\n",
        "        df['VIX_Regime'] = (df['INDIAVIX'] > df['VIX_MA10']).astype(int)\n",
        "\n",
        "    if 'USDINR' in df.columns:\n",
        "        df['Currency_Return'] = df['USDINR'].pct_change()\n",
        "        df['Currency_MA'] = df['USDINR'].rolling(20, min_periods=10).mean()\n",
        "        df['Currency_Strength'] = (df['USDINR'] - df['Currency_MA']) / (df['Currency_MA'] + 1e-8)\n",
        "\n",
        "    if 'SP500' in df.columns:\n",
        "        df['SP500_Return'] = df['SP500'].pct_change()\n",
        "        df['SP500_Corr_30d'] = df['Return_1d'].rolling(30, min_periods=20).corr(df['SP500_Return'])\n",
        "        df['SP500_Corr_30d'] = df['SP500_Corr_30d'].fillna(0)\n",
        "        df['SP500_Lead'] = df['SP500'].pct_change().shift(1)\n",
        "\n",
        "    if 'OIL' in df.columns:\n",
        "        df['Oil_Return'] = df['OIL'].pct_change()\n",
        "        df['Oil_Momentum_5d'] = df['Oil_Return'].rolling(5, min_periods=3).mean()\n",
        "\n",
        "    if 'HSI' in df.columns:\n",
        "        df['HSI_Return'] = df['HSI'].pct_change()\n",
        "        df['Asia_Lead'] = df['HSI_Return'].shift(1)\n",
        "\n",
        "    if 'US10Y' in df.columns:\n",
        "        df['Yield_Level'] = df['US10Y']\n",
        "        df['Yield_Change'] = df['US10Y'].diff()\n",
        "\n",
        "    print(\"  ‚Ä¢ Lagged features...\")\n",
        "    for lag in [1, 2, 3, 5, 10]:\n",
        "        df[f'Return_Lag{lag}'] = df['Return_1d'].shift(lag)\n",
        "        if 'Volume_Ratio' in df.columns:\n",
        "            df[f'Volume_Lag{lag}'] = df['Volume_Ratio'].shift(lag)\n",
        "\n",
        "    df['Momentum_5d'] = df['Close'].pct_change(5)\n",
        "    df['Momentum_10d'] = df['Close'].pct_change(10)\n",
        "    df['Momentum_20d'] = df['Close'].pct_change(20)\n",
        "\n",
        "    print(\"  ‚Ä¢ Regime features...\")\n",
        "    df['SMA_50'] = df['Close'].rolling(50).mean()\n",
        "    df['SMA_200'] = df['Close'].rolling(200).mean()\n",
        "    df['Trend_Regime'] = ((df['Close'] > df['SMA_50']) & (df['SMA_50'] > df['SMA_200'])).astype(int)\n",
        "    df['Vol_Regime'] = (df['RealVol_20d'] > df['RealVol_20d'].rolling(100, min_periods=50).median()).astype(int)\n",
        "    df['Daily_Range'] = (df['High'] - df['Low']) / (df['Close'] + 1e-8)\n",
        "    df['Range_Regime'] = (df['Daily_Range'] > df['Daily_Range'].rolling(50, min_periods=25).median()).astype(int)\n",
        "\n",
        "    print(\"  ‚Ä¢ High-performance features...\")\n",
        "\n",
        "    if 'SP500' in df.columns and 'SP500_Return' in df.columns:\n",
        "        df['SP500_Corr_60d'] = df['Return_1d'].rolling(60, min_periods=40).corr(df['SP500_Return'])\n",
        "        df['SP500_Corr_60d'] = df['SP500_Corr_60d'].fillna(0)\n",
        "\n",
        "    if 'SMA_50' in df.columns and 'SMA_200' in df.columns:\n",
        "        df['Trend_Signal'] = (df['SMA_50'] / (df['SMA_200'] + 1e-8) - 1)\n",
        "        trend_signal_change = df['Trend_Signal'].pct_change()\n",
        "        df['Corr_Trend'] = df['Return_1d'].rolling(30, min_periods=20).corr(trend_signal_change)\n",
        "        df['Corr_Trend'] = df['Corr_Trend'].fillna(0)\n",
        "        df['RelStrength_Trend'] = df['Close'] / (df['SMA_50'] + 1e-8)\n",
        "\n",
        "    if 'INDIAVIX' in df.columns:\n",
        "        vix_ma60 = df['INDIAVIX'].rolling(60, min_periods=30).mean()\n",
        "        df['VIX_Ratio'] = df['INDIAVIX'] / (vix_ma60 + 1)\n",
        "        df['VIX_Acceleration'] = df['VIX_Change'].diff()\n",
        "\n",
        "    if 'USDINR' in df.columns and 'Currency_Return' in df.columns:\n",
        "        df['Currency_Vol'] = df['Currency_Return'].rolling(20, min_periods=10).std()\n",
        "        df['Currency_Momentum'] = df['USDINR'].pct_change(10)\n",
        "\n",
        "    if 'OIL' in df.columns and 'Oil_Return' in df.columns:\n",
        "        df['Oil_Nifty_Corr'] = df['Return_1d'].rolling(30, min_periods=20).corr(df['Oil_Return'])\n",
        "        df['Oil_Nifty_Corr'] = df['Oil_Nifty_Corr'].fillna(0)\n",
        "\n",
        "    print(\"  ‚úÖ All features created\\n\")\n",
        "\n",
        "    return df\n",
        "\n",
        "df_enhanced = create_advanced_features(df_enhanced)\n",
        "\n",
        "nifty_data_original = nifty_data.copy()\n",
        "nifty_data = df_enhanced.copy()\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(f\"‚úÖ ENHANCED DATASET READY V2\")\n",
        "print(f\"   Original features: {nifty_data_original.shape[1]}\")\n",
        "print(f\"   Enhanced features: {nifty_data.shape[1]}\")\n",
        "print(f\"   New features added: {nifty_data.shape[1] - nifty_data_original.shape[1]}\")\n",
        "print(f\"\\nüìä Expected usable features in Cell 8: ~{nifty_data.shape[1] - 15}\")\n",
        "print(\"=\"*70 + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AjEvOB2OY8U0",
        "outputId": "0fd64d3b-c60c-44bc-c5dc-7b041dfac32b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚è∞ TEMPORAL SPLIT - LEAK-PROOF APPROACH\n",
            "==================================================\n",
            "üìÖ Defining temporal split boundaries...\n",
            "Data range: 2018-12-03 to 2025-12-03\n",
            "Train period: 2018-12-03 to 2023-12-03\n",
            "Validation period: 2023-12-03 to 2024-12-03\n",
            "Test period: 2024-12-03 to 2025-12-03\n",
            "\n",
            "üéØ Splitting primary asset (Nifty 50) temporally...\n",
            "‚úÖ Primary asset splits:\n",
            "   Train: 1,305 samples (71.4%)\n",
            "   Validation: 262 samples (14.3%)\n",
            "   Test: 261 samples (14.3%)\n",
            "\n",
            "üåê Splitting multi-asset data temporally...\n",
            "‚úÖ Multi-asset splits:\n",
            "   Train: 11,745 samples\n",
            "   Validation: 2,358 samples\n",
            "   Test: 2,349 samples\n",
            "\n",
            "üîç Verifying no temporal leakage...\n",
            "Train ends: 2023-12-01\n",
            "Val starts: 2023-12-04\n",
            "Val ends: 2024-12-03\n",
            "Test starts: 2024-12-04\n",
            "‚úÖ NO TEMPORAL LEAKAGE: Clean chronological splits\n",
            "\n",
            "‚úÖ TEMPORAL SPLIT COMPLETE!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(\"‚è∞ TEMPORAL SPLIT - LEAK-PROOF APPROACH\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "\n",
        "print(\"üìÖ Defining temporal split boundaries...\")\n",
        "\n",
        "\n",
        "data_start = nifty_data['Date'].min()\n",
        "data_end = nifty_data['Date'].max()\n",
        "\n",
        "\n",
        "train_end_date = data_start + pd.DateOffset(years=5)\n",
        "val_end_date = data_start + pd.DateOffset(years=6)\n",
        "test_end_date = data_end\n",
        "\n",
        "print(f\"Data range: {data_start.strftime('%Y-%m-%d')} to {data_end.strftime('%Y-%m-%d')}\")\n",
        "print(f\"Train period: {data_start.strftime('%Y-%m-%d')} to {train_end_date.strftime('%Y-%m-%d')}\")\n",
        "print(f\"Validation period: {train_end_date.strftime('%Y-%m-%d')} to {val_end_date.strftime('%Y-%m-%d')}\")\n",
        "print(f\"Test period: {val_end_date.strftime('%Y-%m-%d')} to {test_end_date.strftime('%Y-%m-%d')}\")\n",
        "\n",
        "# ===== STEP 2: SPLIT PRIMARY ASSET DATA =====\n",
        "print(\"\\nüéØ Splitting primary asset (Nifty 50) temporally...\")\n",
        "\n",
        "train_data = nifty_data[nifty_data['Date'] <= train_end_date].copy()\n",
        "val_data = nifty_data[(nifty_data['Date'] > train_end_date) &\n",
        "                      (nifty_data['Date'] <= val_end_date)].copy()\n",
        "test_data = nifty_data[nifty_data['Date'] > val_end_date].copy()\n",
        "\n",
        "print(f\"‚úÖ Primary asset splits:\")\n",
        "print(f\"   Train: {len(train_data):,} samples ({len(train_data)*100/len(nifty_data):.1f}%)\")\n",
        "print(f\"   Validation: {len(val_data):,} samples ({len(val_data)*100/len(nifty_data):.1f}%)\")\n",
        "print(f\"   Test: {len(test_data):,} samples ({len(test_data)*100/len(nifty_data):.1f}%)\")\n",
        "\n",
        "# ===== STEP 3: SPLIT MULTI-ASSET DATA FOR TRANSFER LEARNING =====\n",
        "print(\"\\nüåê Splitting multi-asset data temporally...\")\n",
        "\n",
        "multi_train_data = multi_asset_df[multi_asset_df['Date'] <= train_end_date].copy()\n",
        "multi_val_data = multi_asset_df[(multi_asset_df['Date'] > train_end_date) &\n",
        "                                (multi_asset_df['Date'] <= val_end_date)].copy()\n",
        "multi_test_data = multi_asset_df[multi_asset_df['Date'] > val_end_date].copy()\n",
        "\n",
        "print(f\"‚úÖ Multi-asset splits:\")\n",
        "print(f\"   Train: {len(multi_train_data):,} samples\")\n",
        "print(f\"   Validation: {len(multi_val_data):,} samples\")\n",
        "print(f\"   Test: {len(multi_test_data):,} samples\")\n",
        "\n",
        "# ===== STEP 4: VERIFY NO TEMPORAL LEAKAGE =====\n",
        "print(\"\\nüîç Verifying no temporal leakage...\")\n",
        "\n",
        "train_max_date = train_data['Date'].max()\n",
        "val_min_date = val_data['Date'].min()\n",
        "val_max_date = val_data['Date'].max()\n",
        "test_min_date = test_data['Date'].min()\n",
        "\n",
        "print(f\"Train ends: {train_max_date.strftime('%Y-%m-%d')}\")\n",
        "print(f\"Val starts: {val_min_date.strftime('%Y-%m-%d')}\")\n",
        "print(f\"Val ends: {val_max_date.strftime('%Y-%m-%d')}\")\n",
        "print(f\"Test starts: {test_min_date.strftime('%Y-%m-%d')}\")\n",
        "\n",
        "# Check for gaps\n",
        "gap1 = (val_min_date - train_max_date).days\n",
        "gap2 = (test_min_date - val_max_date).days\n",
        "\n",
        "if gap1 >= 0 and gap2 >= 0:\n",
        "    print(\"‚úÖ NO TEMPORAL LEAKAGE: Clean chronological splits\")\n",
        "else:\n",
        "    print(\"‚ùå WARNING: Possible temporal overlap detected!\")\n",
        "\n",
        "print(\"\\n‚úÖ TEMPORAL SPLIT COMPLETE!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WrH9TnBfaKx8",
        "outputId": "9aaf9841-fec1-41d4-f279-d52111f7137f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîß ENHANCED FEATURE ENGINEERING WITH EXTERNAL SIGNALS\n",
            "=================================================================\n",
            "üìä Applying enhanced features to all data splits...\n",
            "üìà Creating original technical features...\n",
            "üåç Enhancing external signal features...\n",
            "üîó Creating interaction features...\n",
            "üìä Creating regime-based features...\n",
            "üìà Creating original technical features...\n",
            "üåç Enhancing external signal features...\n",
            "üîó Creating interaction features...\n",
            "üìä Creating regime-based features...\n",
            "üìà Creating original technical features...\n",
            "üåç Enhancing external signal features...\n",
            "üîó Creating interaction features...\n",
            "üìä Creating regime-based features...\n",
            "‚úÖ Enhanced features applied:\n",
            "   Train: (1305, 92)\n",
            "   Validation: (262, 92)\n",
            "   Test: (261, 92)\n",
            "\n",
            "üéØ Feature selection:\n",
            "   Total feature columns: 85\n",
            "   External signal features: 15\n",
            "‚úÖ Enhanced feature matrices created:\n",
            "   Train: (1305, 85)\n",
            "   Validation: (262, 85)\n",
            "   Test: (261, 85)\n",
            "\n",
            "‚úÖ ENHANCED FEATURE ENGINEERING COMPLETE!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "print(\"üîß ENHANCED FEATURE ENGINEERING WITH EXTERNAL SIGNALS\")\n",
        "print(\"=\"*65)\n",
        "\n",
        "def create_comprehensive_features(df):\n",
        "    \"\"\"\n",
        "    Create comprehensive feature set including external market signals\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "\n",
        "    # ===== ORIGINAL TECHNICAL FEATURES =====\n",
        "    print(\"üìà Creating original technical features...\")\n",
        "\n",
        "    # Price-based features\n",
        "    df['Returns'] = df['Close'].pct_change()\n",
        "    df['Log_Returns'] = np.log(df['Close'] / df['Close'].shift(1))\n",
        "    df['Price_Change'] = df['Close'] - df['Open']\n",
        "    df['Daily_Range'] = df['High'] - df['Low']\n",
        "    df['Body_Size'] = abs(df['Close'] - df['Open'])\n",
        "\n",
        "    # Moving averages\n",
        "    df['SMA_5'] = df['Close'].rolling(5, min_periods=1).mean()\n",
        "    df['SMA_10'] = df['Close'].rolling(10, min_periods=1).mean()\n",
        "    df['SMA_20'] = df['Close'].rolling(20, min_periods=1).mean()\n",
        "    df['SMA_50'] = df['Close'].rolling(50, min_periods=1).mean()\n",
        "\n",
        "    # Exponential moving averages\n",
        "    df['EMA_12'] = df['Close'].ewm(span=12).mean()\n",
        "    df['EMA_26'] = df['Close'].ewm(span=26).mean()\n",
        "\n",
        "    # MACD\n",
        "    df['MACD'] = df['EMA_12'] - df['EMA_26']\n",
        "    df['MACD_Signal'] = df['MACD'].ewm(span=9).mean()\n",
        "    df['MACD_Hist'] = df['MACD'] - df['MACD_Signal']  # FIXED: Added closing bracket\n",
        "\n",
        "    # RSI\n",
        "    delta = df['Close'].diff()\n",
        "    gain = delta.where(delta > 0, 0).rolling(14, min_periods=1).mean()\n",
        "    loss = (-delta.where(delta < 0, 0)).rolling(14, min_periods=1).mean()\n",
        "    rs = gain / loss.replace(0, 0.0001)\n",
        "    df['RSI_14'] = 100 - (100 / (1 + rs))\n",
        "\n",
        "    # Bollinger Bands\n",
        "    sma_20 = df['SMA_20']\n",
        "    std_20 = df['Close'].rolling(20, min_periods=1).std()\n",
        "    df['BB_upper'] = sma_20 + (2 * std_20)\n",
        "    df['BB_lower'] = sma_20 - (2 * std_20)\n",
        "    df['BB_width'] = df['BB_upper'] - df['BB_lower']\n",
        "    df['BB_position'] = (df['Close'] - df['BB_lower']) / (df['BB_upper'] - df['BB_lower'])\n",
        "\n",
        "    # ===== ENHANCED EXTERNAL FEATURES =====\n",
        "    print(\"üåç Enhancing external signal features...\")\n",
        "\n",
        "    # Market regime detection based on multiple signals\n",
        "    if 'Vol_Regime' in df.columns and 'VIX_Ratio' in df.columns:\n",
        "        # Combined volatility signal\n",
        "        df['Market_Stress'] = (df['Vol_Regime'] == 2).astype(int) * df['VIX_Ratio']\n",
        "\n",
        "    # Inter-market momentum features\n",
        "    if 'Corr_SP500_30' in df.columns:\n",
        "        # Correlation stability (low = potential regime change)\n",
        "        df['Corr_Stability'] = df['Corr_SP500_30'].rolling(10).std()\n",
        "\n",
        "        # Correlation regime\n",
        "        corr_percentile = df['Corr_SP500_30'].rolling(126).rank(pct=True)  # 6-month percentile\n",
        "        df['Corr_Regime'] = np.where(corr_percentile > 0.8, 2,  # High correlation\n",
        "                                   np.where(corr_percentile < 0.2, 0, 1))  # Low, Normal, High\n",
        "\n",
        "    # Relative strength momentum\n",
        "    if 'RelStrength_MSCI' in df.columns:\n",
        "        df['RelStrength_MA'] = df['RelStrength_MSCI'].rolling(20).mean()\n",
        "        df['RelStrength_Signal'] = np.where(df['RelStrength_MSCI'] > df['RelStrength_MA'], 1, -1)\n",
        "\n",
        "    # Currency and commodity combined impact\n",
        "    if 'INR_Strength' in df.columns and 'Oil_Momentum' in df.columns:\n",
        "        # Combined external pressure (strong INR + low oil = positive for equities)\n",
        "        df['External_Pressure'] = df['INR_Strength'] - df['Oil_Momentum']\n",
        "\n",
        "    # Market breadth features\n",
        "    if 'AD_Line' in df.columns:\n",
        "        # A/D Line momentum\n",
        "        df['AD_Momentum'] = df['AD_Line'] - df['AD_Line'].shift(10)\n",
        "\n",
        "        # Breadth divergence (price up but breadth down = bearish)\n",
        "        price_trend = df['Close'].pct_change(10)\n",
        "        ad_trend = df['AD_Line'].pct_change(10)\n",
        "        df['Breadth_Divergence'] = price_trend - ad_trend\n",
        "\n",
        "    # Sentiment features\n",
        "    if 'PCR' in df.columns:\n",
        "        # PCR moving average for trend\n",
        "        df['PCR_MA'] = df['PCR'].rolling(10).mean()\n",
        "\n",
        "        # Extreme sentiment readings\n",
        "        df['Extreme_Fear'] = (df['PCR'] > 1.3).astype(int)  # Contrarian bullish\n",
        "        df['Extreme_Greed'] = (df['PCR'] < 0.7).astype(int)  # Contrarian bearish\n",
        "\n",
        "    # ===== INTERACTION FEATURES =====\n",
        "    print(\"üîó Creating interaction features...\")\n",
        "\n",
        "    # Technical + External interactions\n",
        "    if 'VIX_Ratio' in df.columns:\n",
        "        # RSI effectiveness in different volatility regimes\n",
        "        df['RSI_Vol_Adjusted'] = df['RSI_14'] * (1 / (1 + df['VIX_Ratio']))\n",
        "\n",
        "    if 'Corr_SP500_30' in df.columns:\n",
        "        # MACD effectiveness based on correlation regime\n",
        "        df['MACD_Corr_Adjusted'] = df['MACD_Hist'] * (1 + abs(df['Corr_SP500_30']))\n",
        "\n",
        "    # ===== REGIME-BASED FEATURES =====\n",
        "    print(\"üìä Creating regime-based features...\")\n",
        "\n",
        "    # Market regime classification (combine multiple signals)\n",
        "    regime_score = 0\n",
        "\n",
        "    if 'Vol_Regime' in df.columns:\n",
        "        regime_score += df['Vol_Regime']  # 0-2\n",
        "\n",
        "    if 'Corr_Regime' in df.columns:\n",
        "        regime_score += df['Corr_Regime']  # 0-2\n",
        "\n",
        "    if 'PCR_Signal' in df.columns:\n",
        "        regime_score += (df['PCR_Signal'] + 1)  # -1,0,1 -> 0,1,2\n",
        "\n",
        "    # Overall market regime (0=calm, higher=stressed)\n",
        "    df['Market_Regime'] = regime_score\n",
        "\n",
        "    # Fill NaN and infinite values\n",
        "    df = df.fillna(0)\n",
        "    df = df.replace([np.inf, -np.inf], 0)\n",
        "\n",
        "    return df\n",
        "\n",
        "# Apply comprehensive feature engineering to train, val, and test data\n",
        "print(\"üìä Applying enhanced features to all data splits...\")\n",
        "\n",
        "train_features_df = create_comprehensive_features(train_data)\n",
        "val_features_df = create_comprehensive_features(val_data)\n",
        "test_features_df = create_comprehensive_features(test_data)\n",
        "\n",
        "print(f\"‚úÖ Enhanced features applied:\")\n",
        "print(f\"   Train: {train_features_df.shape}\")\n",
        "print(f\"   Validation: {val_features_df.shape}\")\n",
        "print(f\"   Test: {test_features_df.shape}\")\n",
        "\n",
        "# Select feature columns (exclude non-feature columns)\n",
        "exclude_cols = ['Date', 'Symbol', 'Open', 'High', 'Low', 'Close', 'Volume']\n",
        "feature_columns = [col for col in train_features_df.columns if col not in exclude_cols]\n",
        "\n",
        "print(f\"\\nüéØ Feature selection:\")\n",
        "print(f\"   Total feature columns: {len(feature_columns)}\")\n",
        "print(f\"   External signal features: {len([col for col in feature_columns if any(x in col for x in ['Corr_', 'VIX_', 'RelStrength', 'INR_', 'Oil_', 'AD_', 'PCR', 'Market_', 'External_'])])}\")\n",
        "\n",
        "# Extract feature matrices\n",
        "X_train_raw = train_features_df[feature_columns].values\n",
        "X_val_raw = val_features_df[feature_columns].values\n",
        "X_test_raw = test_features_df[feature_columns].values\n",
        "\n",
        "print(f\"‚úÖ Enhanced feature matrices created:\")\n",
        "print(f\"   Train: {X_train_raw.shape}\")\n",
        "print(f\"   Validation: {X_val_raw.shape}\")\n",
        "print(f\"   Test: {X_test_raw.shape}\")\n",
        "\n",
        "print(\"\\n‚úÖ ENHANCED FEATURE ENGINEERING COMPLETE!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "YCJJz2p0Ri6-",
        "outputId": "f7de1139-843d-4f33-d1ac-7b8229834f6e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sUSgKp0PcESI"
      },
      "outputs": [],
      "source": [
        "\n",
        "print(\"üéØ DIRECTIONAL LABEL CREATION WITH DEAD ZONE\")\n",
        "print(\"=\"*55)\n",
        "\n",
        "\n",
        "threshold = 0.001\n",
        "\n",
        "print(f\"Dead zone threshold: ¬±{threshold*100:.1f}%\")\n",
        "print(\"Rationale: Covers transaction costs + noise filtering\")\n",
        "\n",
        "\n",
        "def calculate_next_return(df):\n",
        "    \"\"\"Calculate next-day returns\"\"\"\n",
        "    df = df.copy()\n",
        "    df['Next_Close'] = df['Close'].shift(-1)\n",
        "    df['Next_Return'] = (df['Next_Close'] - df['Close']) / df['Close']\n",
        "    return df\n",
        "\n",
        "# Apply to all datasets\n",
        "print(\"\\nüìà Calculating next-day returns...\")\n",
        "\n",
        "train_with_returns = calculate_next_return(train_features_df)\n",
        "val_with_returns = calculate_next_return(val_features_df)\n",
        "test_with_returns = calculate_next_return(test_features_df)\n",
        "\n",
        "\n",
        "def create_directional_labels(returns, threshold):\n",
        "    \"\"\"\n",
        "    Create directional labels with dead zone\n",
        "    1 = UP (return > +threshold)\n",
        "    0 = DOWN (return < -threshold)\n",
        "    NaN = DEAD ZONE (within ¬±threshold) - to be dropped\n",
        "    \"\"\"\n",
        "    labels = np.where(returns > threshold, 1,\n",
        "                     np.where(returns < -threshold, 0, np.nan))\n",
        "    return labels\n",
        "\n",
        "print(f\"\\nüè∑Ô∏è Creating directional labels...\")\n",
        "\n",
        "y_train_raw = create_directional_labels(train_with_returns['Next_Return'].values, threshold)\n",
        "y_val_raw = create_directional_labels(val_with_returns['Next_Return'].values, threshold)\n",
        "y_test_raw = create_directional_labels(test_with_returns['Next_Return'].values, threshold)\n",
        "\n",
        "# ===== STEP 4: REMOVE DEAD ZONE SAMPLES =====\n",
        "print(\"üóëÔ∏è Removing dead zone samples...\")\n",
        "\n",
        "# Find valid indices (non-NaN labels)\n",
        "train_valid_idx = ~np.isnan(y_train_raw)\n",
        "val_valid_idx = ~np.isnan(y_val_raw)\n",
        "test_valid_idx = ~np.isnan(y_test_raw)\n",
        "\n",
        "# Filter features and labels\n",
        "X_train_filtered = X_train_raw[train_valid_idx]\n",
        "y_train_filtered = y_train_raw[train_valid_idx].astype(int)\n",
        "\n",
        "X_val_filtered = X_val_raw[val_valid_idx]\n",
        "y_val_filtered = y_val_raw[val_valid_idx].astype(int)\n",
        "\n",
        "X_test_filtered = X_test_raw[test_valid_idx]\n",
        "y_test_filtered = y_test_raw[test_valid_idx].astype(int)\n",
        "\n",
        "print(f\"‚úÖ Filtered datasets:\")\n",
        "print(f\"   Train: {X_train_raw.shape[0]} ‚Üí {X_train_filtered.shape[0]} ({np.mean(train_valid_idx)*100:.1f}% kept)\")\n",
        "print(f\"   Validation: {X_val_raw.shape[0]} ‚Üí {X_val_filtered.shape[0]} ({np.mean(val_valid_idx)*100:.1f}% kept)\")\n",
        "print(f\"   Test: {X_test_raw.shape[0]} ‚Üí {X_test_filtered.shape[0]} ({np.mean(test_valid_idx)*100:.1f}% kept)\")\n",
        "\n",
        "# ===== STEP 5: CLASS DISTRIBUTION ANALYSIS =====\n",
        "print(f\"\\nüìä Class distribution analysis:\")\n",
        "\n",
        "for split_name, labels in [(\"Train\", y_train_filtered), (\"Val\", y_val_filtered), (\"Test\", y_test_filtered)]:\n",
        "    up_count = np.sum(labels == 1)\n",
        "    down_count = np.sum(labels == 0)\n",
        "    total = len(labels)\n",
        "    up_pct = up_count / total * 100\n",
        "    down_pct = down_count / total * 100\n",
        "\n",
        "    print(f\"   {split_name}: UP={up_count} ({up_pct:.1f}%), DOWN={down_count} ({down_pct:.1f}%)\")\n",
        "\n",
        "# Calculate class weights for imbalanced learning\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "class_weights = compute_class_weight('balanced',\n",
        "                                   classes=np.array([0, 1]),\n",
        "                                   y=y_train_filtered)\n",
        "class_weight_dict = {0: class_weights[0], 1: class_weights[1]}\n",
        "\n",
        "print(f\"\\n‚öñÔ∏è Computed class weights:\")\n",
        "print(f\"   DOWN (0): {class_weights[0]:.3f}\")\n",
        "print(f\"   UP (1): {class_weights[1]:.3f}\")\n",
        "\n",
        "print(\"\\n‚úÖ DIRECTIONAL LABELS WITH DEAD ZONE COMPLETE!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KdkBI3jTdaPn"
      },
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.feature_selection import SelectKBest, mutual_info_classif, f_classif\n",
        "\n",
        "print(\"‚öñÔ∏è FEATURE SCALING & SELECTION - LEAK-PROOF\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# ===== STEP 1: FIT SCALER ONLY ON TRAIN DATA =====\n",
        "print(\"üîß Fitting RobustScaler on train data only...\")\n",
        "\n",
        "# Use RobustScaler to handle outliers better than StandardScaler\n",
        "feature_scaler = RobustScaler()\n",
        "\n",
        "# FIT only on train data\n",
        "X_train_scaled = feature_scaler.fit_transform(X_train_filtered)\n",
        "\n",
        "# TRANSFORM val and test (no fitting)\n",
        "X_val_scaled = feature_scaler.transform(X_val_filtered)\n",
        "X_test_scaled = feature_scaler.transform(X_test_filtered)\n",
        "\n",
        "print(f\"‚úÖ Scaling complete:\")\n",
        "print(f\"   Feature scaler fitted on {X_train_scaled.shape[0]} train samples\")\n",
        "print(f\"   Applied to all splits\")\n",
        "\n",
        "# ===== STEP 2: FEATURE SELECTION ON TRAIN DATA ONLY =====\n",
        "print(f\"\\nüéØ Feature selection using mutual information (train only)...\")\n",
        "\n",
        "# Use mutual information for feature selection (good for non-linear relationships)\n",
        "selector = SelectKBest(mutual_info_classif, k=20)  # Select top 20 features\n",
        "\n",
        "# FIT selector only on train data\n",
        "X_train_selected = selector.fit_transform(X_train_scaled, y_train_filtered)\n",
        "\n",
        "# TRANSFORM val and test using fitted selector\n",
        "X_val_selected = selector.transform(X_val_scaled)\n",
        "X_test_selected = selector.transform(X_test_scaled)\n",
        "\n",
        "# Get selected feature names\n",
        "selected_features = [feature_columns[i] for i in selector.get_support(indices=True)]\n",
        "feature_scores = selector.scores_\n",
        "\n",
        "print(f\"‚úÖ Feature selection complete:\")\n",
        "print(f\"   Selected {X_train_selected.shape[1]} features from {X_train_scaled.shape[1]}\")\n",
        "print(f\"   Selection fitted on train data only\")\n",
        "\n",
        "print(f\"\\nüèÜ Top 10 selected features:\")\n",
        "feature_importance = list(zip(selected_features, feature_scores[selector.get_support()]))\n",
        "feature_importance.sort(key=lambda x: x[1], reverse=True)\n",
        "for i, (feature, score) in enumerate(feature_importance[:10]):\n",
        "    print(f\"   {i+1:2d}. {feature:<20} (score: {score:.4f})\")\n",
        "\n",
        "\n",
        "print(f\"\\nüìä Final processed datasets:\")\n",
        "print(f\"   Train: {X_train_selected.shape} features, {len(y_train_filtered)} labels\")\n",
        "print(f\"   Validation: {X_val_selected.shape} features, {len(y_val_filtered)} labels\")\n",
        "print(f\"   Test: {X_test_selected.shape} features, {len(y_test_filtered)} labels\")\n",
        "\n",
        "\n",
        "train_dates = train_with_returns[train_valid_idx]['Date'].values\n",
        "val_dates = val_with_returns[val_valid_idx]['Date'].values\n",
        "test_dates = test_with_returns[test_valid_idx]['Date'].values\n",
        "\n",
        "print(f\"\\n‚úÖ Date information preserved for sequence creation\")\n",
        "\n",
        "print(\"\\n‚úÖ FEATURE SCALING & SELECTION COMPLETE!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7tXvXfqrdfEZ"
      },
      "outputs": [],
      "source": [
        "\n",
        "print(\"üì± SEQUENCE CREATION WITH STRIDE\")\n",
        "print(\"=\"*40)\n",
        "\n",
        "\n",
        "def create_sequences_with_stride(X, y, dates, sequence_length=30, stride=3):\n",
        "    \"\"\"\n",
        "    Create sequences with stride > 1 to reduce redundancy\n",
        "\n",
        "    Args:\n",
        "        X: Feature matrix\n",
        "        y: Labels\n",
        "        dates: Date array\n",
        "        sequence_length: Length of each sequence\n",
        "        stride: Step size between sequences (>1 to reduce overlap)\n",
        "\n",
        "    Returns:\n",
        "        X_sequences, y_sequences, sequence_dates\n",
        "    \"\"\"\n",
        "    sequences_X = []\n",
        "    sequences_y = []\n",
        "    sequences_dates = []\n",
        "\n",
        "\n",
        "    for i in range(sequence_length, len(X), stride):\n",
        "\n",
        "        seq_x = X[i-sequence_length:i]\n",
        "        seq_y = y[i]  # Next day label\n",
        "        seq_date = dates[i]  # Date of prediction\n",
        "\n",
        "        sequences_X.append(seq_x)\n",
        "        sequences_y.append(seq_y)\n",
        "        sequences_dates.append(seq_date)\n",
        "\n",
        "    return (np.array(sequences_X),\n",
        "            np.array(sequences_y),\n",
        "            np.array(sequences_dates))\n",
        "\n",
        "\n",
        "sequence_length = 30\n",
        "stride = 3\n",
        "\n",
        "print(f\"Sequence parameters:\")\n",
        "print(f\"   Sequence length: {sequence_length} days\")\n",
        "print(f\"   Stride: {stride} (reduces samples by ~{stride}x)\")\n",
        "print(f\"   Redundancy reduction: {(1-1/stride)*100:.1f}%\")\n",
        "\n",
        "# ===== STEP 3: CREATE SEQUENCES FOR ALL SPLITS =====\n",
        "print(f\"\\nüîÑ Creating sequences with stride...\")\n",
        "\n",
        "# Train sequences\n",
        "X_train_seq, y_train_seq, train_seq_dates = create_sequences_with_stride(\n",
        "    X_train_selected, y_train_filtered, train_dates, sequence_length, stride\n",
        ")\n",
        "\n",
        "# Validation sequences\n",
        "X_val_seq, y_val_seq, val_seq_dates = create_sequences_with_stride(\n",
        "    X_val_selected, y_val_filtered, val_dates, sequence_length, stride\n",
        ")\n",
        "\n",
        "# Test sequences\n",
        "X_test_seq, y_test_seq, test_seq_dates = create_sequences_with_stride(\n",
        "    X_test_selected, y_test_filtered, test_dates, sequence_length, stride\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Sequences created:\")\n",
        "print(f\"   Train: {X_train_seq.shape} ‚Üí {len(y_train_seq)} labels\")\n",
        "print(f\"   Validation: {X_val_seq.shape} ‚Üí {len(y_val_seq)} labels\")\n",
        "print(f\"   Test: {X_test_seq.shape} ‚Üí {len(y_test_seq)} labels\")\n",
        "\n",
        "# ===== STEP 4: SEQUENCE VALIDATION =====\n",
        "print(f\"\\nüîç Sequence validation:\")\n",
        "\n",
        "# Check for temporal consistency\n",
        "print(\"Temporal consistency checks:\")\n",
        "for name, dates_array in [(\"Train\", train_seq_dates), (\"Val\", val_seq_dates), (\"Test\", test_seq_dates)]:\n",
        "    if len(dates_array) > 0:\n",
        "        min_date = pd.to_datetime(dates_array.min())\n",
        "        max_date = pd.to_datetime(dates_array.max())\n",
        "        print(f\"   {name}: {min_date.strftime('%Y-%m-%d')} to {max_date.strftime('%Y-%m-%d')}\")\n",
        "\n",
        "# Check class distribution in sequences\n",
        "print(f\"\\nClass distribution in sequences:\")\n",
        "for name, labels in [(\"Train\", y_train_seq), (\"Val\", y_val_seq), (\"Test\", y_test_seq)]:\n",
        "    if len(labels) > 0:\n",
        "        up_pct = np.mean(labels == 1) * 100\n",
        "        down_pct = np.mean(labels == 0) * 100\n",
        "        print(f\"   {name}: UP={up_pct:.1f}%, DOWN={down_pct:.1f}%\")\n",
        "\n",
        "# Calculate reduction in samples due to stride\n",
        "original_possible_sequences = len(X_train_selected) + len(X_val_selected) + len(X_test_selected) - 3*sequence_length\n",
        "actual_sequences = len(y_train_seq) + len(y_val_seq) + len(y_test_seq)\n",
        "reduction_factor = original_possible_sequences / actual_sequences if actual_sequences > 0 else 0\n",
        "\n",
        "print(f\"\\nüìä Sequence efficiency:\")\n",
        "print(f\"   Possible sequences (stride=1): ~{original_possible_sequences}\")\n",
        "print(f\"   Actual sequences (stride={stride}): {actual_sequences}\")\n",
        "print(f\"   Reduction factor: {reduction_factor:.1f}x\")\n",
        "print(f\"   Memory/computation savings: {(1-1/reduction_factor)*100:.1f}%\")\n",
        "\n",
        "print(\"\\n‚úÖ SEQUENCE CREATION WITH STRIDE COMPLETE!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dI0Mz0X1eMis"
      },
      "outputs": [],
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Model, regularizers\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "from tensorflow.keras.metrics import AUC\n",
        "\n",
        "print(\"üèóÔ∏è ENHANCED MODEL ARCHITECTURE WITH STACKED LSTMS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "\n",
        "def build_enhanced_lstm_cnn_classifier(input_shape):\n",
        "    \"\"\"\n",
        "    Build enhanced LSTM-CNN hybrid with stacked architecture\n",
        "    \"\"\"\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "\n",
        "\n",
        "    x = layers.Conv1D(filters=64, kernel_size=5, padding='same',\n",
        "                      kernel_regularizer=regularizers.l2(1e-4))(inputs)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.ReLU()(x)\n",
        "    x = layers.SpatialDropout1D(0.15)(x)  # Reduced from 0.2\n",
        "\n",
        "    # Second CNN block\n",
        "    x = layers.Conv1D(filters=32, kernel_size=3, padding='same',\n",
        "                      kernel_regularizer=regularizers.l2(1e-4))(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.ReLU()(x)\n",
        "    x = layers.SpatialDropout1D(0.15)(x)  # Reduced from 0.2\n",
        "\n",
        "    # ===== STACKED LSTM LAYERS =====\n",
        "    # First LSTM layer (return sequences for stacking)\n",
        "    x = layers.LSTM(units=128,  # Doubled from 64\n",
        "                    dropout=0.15,  # Reduced from 0.3\n",
        "                    recurrent_dropout=0.1,  # Reduced from 0.2\n",
        "                    kernel_regularizer=regularizers.l2(1e-4),\n",
        "                    return_sequences=True)(x)  # Keep sequences for stacking\n",
        "\n",
        "\n",
        "    x = layers.LSTM(units=128,  # Doubled from 64\n",
        "                    dropout=0.15,  # Reduced from 0.3\n",
        "                    recurrent_dropout=0.1,  # Reduced from 0.2\n",
        "                    kernel_regularizer=regularizers.l2(1e-4),\n",
        "                    return_sequences=False)(x)  # Final output\n",
        "\n",
        "\n",
        "    x = layers.Dense(64, activation='relu',  # Doubled from 32\n",
        "                     kernel_regularizer=regularizers.l2(1e-4))(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dropout(0.2)(x)  # Reduced from 0.3\n",
        "\n",
        "    # Second dense block\n",
        "    x = layers.Dense(32, activation='relu',\n",
        "                     kernel_regularizer=regularizers.l2(1e-4))(x)\n",
        "    x = layers.Dropout(0.2)(x)  # Reduced from 0.2\n",
        "\n",
        "    # Output layer\n",
        "    outputs = layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "\n",
        "input_shape = (sequence_length, len(feature_columns))\n",
        "print(f\"Enhanced input shape: {input_shape}\")\n",
        "\n",
        "enhanced_model = build_enhanced_lstm_cnn_classifier(input_shape)\n",
        "\n",
        "print(f\"‚úÖ Enhanced model architecture:\")\n",
        "print(f\"   üîÑ Stacked LSTMs: 128 + 128 units (doubled capacity)\")\n",
        "print(f\"   üìâ Reduced dropout: 0.15-0.2 (prevent underfitting)\")\n",
        "print(f\"   üéØ Enhanced dense: 64 + 32 units\")\n",
        "print(f\"   üìä Parameters: {enhanced_model.count_params():,}\")\n",
        "\n",
        "\n",
        "enhanced_class_weights = {0: 1.3, 1: 0.9}\n",
        "\n",
        "enhanced_model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=5e-4, clipnorm=1.0),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy', 'precision', 'recall', AUC(name='auc')]\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Model compiled with enhanced configuration\")\n",
        "\n",
        "\n",
        "enhanced_callbacks = [\n",
        "    EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=25,  # Increased from 15\n",
        "        restore_best_weights=True,\n",
        "        verbose=1,\n",
        "        min_delta=1e-5\n",
        "    ),\n",
        "    ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.5,\n",
        "        patience=15,  # Increased from 8\n",
        "        min_lr=1e-6,\n",
        "        verbose=1\n",
        "    ),\n",
        "    # Dual checkpointing with enhanced model names\n",
        "    ModelCheckpoint(\n",
        "        'enhanced_model_precision.keras',\n",
        "        monitor='val_precision',\n",
        "        save_best_only=True,\n",
        "        verbose=1,\n",
        "        mode='max'\n",
        "    ),\n",
        "    ModelCheckpoint(\n",
        "        'enhanced_model_auc.keras',\n",
        "        monitor='val_auc',\n",
        "        save_best_only=True,\n",
        "        verbose=1,\n",
        "        mode='max'\n",
        "    )\n",
        "]\n",
        "\n",
        "print(f\"‚úÖ Enhanced callbacks configured:\")\n",
        "print(f\"   üìà Increased early stopping patience: 25 epochs\")\n",
        "print(f\"   üîÑ Increased LR reduction patience: 15 epochs\")\n",
        "print(f\"   üíæ Dual checkpointing maintained\")\n",
        "\n",
        "enhanced_model.summary()\n",
        "\n",
        "print(\"\\n‚úÖ ENHANCED MODEL ARCHITECTURE READY!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZz-rBkmYtml"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7747a80a"
      },
      "outputs": [],
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Model, regularizers\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "from tensorflow.keras.metrics import AUC\n",
        "\n",
        "print(\"üèóÔ∏è ENHANCED MODEL ARCHITECTURE WITH STACKED LSTMS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# ===== STEP 1: ENHANCED MODEL ARCHITECTURE =====\n",
        "def build_enhanced_lstm_cnn_classifier(input_shape):\n",
        "    \"\"\"\n",
        "    Build enhanced LSTM-CNN hybrid with stacked architecture\n",
        "    \"\"\"\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "\n",
        "    # ===== ENHANCED CNN LAYERS =====\n",
        "    # First CNN block\n",
        "    x = layers.Conv1D(filters=64, kernel_size=5, padding='same',\n",
        "                      kernel_regularizer=regularizers.l2(1e-4))(inputs)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.ReLU()(x)\n",
        "    x = layers.SpatialDropout1D(0.15)(x)  # Reduced from 0.2\n",
        "\n",
        "    # Second CNN block\n",
        "    x = layers.Conv1D(filters=32, kernel_size=3, padding='same',\n",
        "                      kernel_regularizer=regularizers.l2(1e-4))(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.ReLU()(x)\n",
        "    x = layers.SpatialDropout1D(0.15)(x)  # Reduced from 0.2\n",
        "\n",
        "\n",
        "    x = layers.LSTM(units=128,  # Doubled from 64\n",
        "                    dropout=0.15,  # Reduced from 0.3\n",
        "                    recurrent_dropout=0.1,  # Reduced from 0.2\n",
        "                    kernel_regularizer=regularizers.l2(1e-4),\n",
        "                    return_sequences=True)(x)  # Keep sequences for stacking\n",
        "\n",
        "\n",
        "    x = layers.LSTM(units=128,  # Doubled from 64\n",
        "                    dropout=0.15,  # Reduced from 0.3\n",
        "                    recurrent_dropout=0.1,  # Reduced from 0.2\n",
        "                    kernel_regularizer=regularizers.l2(1e-4),\n",
        "                    return_sequences=False)(x)  # Final output\n",
        "\n",
        "\n",
        "    x = layers.Dense(64, activation='relu',  # Doubled from 32\n",
        "                     kernel_regularizer=regularizers.l2(1e-4))(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dropout(0.2)(x)  # Reduced from 0.3\n",
        "\n",
        "\n",
        "    x = layers.Dense(32, activation='relu',\n",
        "                     kernel_regularizer=regularizers.l2(1e-4))(x)\n",
        "    x = layers.Dropout(0.2)(x)  # Reduced from 0.2\n",
        "\n",
        "    # Output layer\n",
        "    outputs = layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "\n",
        "# enhanced_model = build_enhanced_lstm_cnn_classifier(input_shape)\n",
        "\n",
        "# print(f\"‚úÖ Enhanced model architecture:\")\n",
        "# print(f\"   üîÑ Stacked LSTMs: 128 + 128 units (doubled capacity)\")\n",
        "# print(f\"   üìâ Reduced dropout: 0.15-0.2 (prevent underfitting)\")\n",
        "# print(f\"   üéØ Enhanced dense: 64 + 32 units\")\n",
        "# print(f\"   üìä Parameters: {enhanced_model.count_params():,}\")\n",
        "\n",
        "# ===== STEP 3: COMPILE WITH OPTIMIZED SETTINGS =====\n",
        "# Safer class weights for enhanced model\n",
        "enhanced_class_weights = {0: 1.3, 1: 0.9}\n",
        "\n",
        "# The model is compiled inside the walk-forward loop with the correct input_shape and optimizer.\n",
        "\n",
        "# ===== STEP 4: ENHANCED CALLBACKS WITH LONGER PATIENCE =====\n",
        "enhanced_callbacks = [\n",
        "    EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=25,  # Increased from 15\n",
        "        restore_best_weights=True,\n",
        "        verbose=1,\n",
        "        min_delta=1e-5\n",
        "    ),\n",
        "    ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.5,\n",
        "        patience=15,  # Increased from 8\n",
        "        min_lr=1e-6,\n",
        "        verbose=1\n",
        "    ),\n",
        "    # Dual checkpointing with enhanced model names\n",
        "    ModelCheckpoint(\n",
        "        'enhanced_model_precision.keras',\n",
        "        monitor='val_precision',\n",
        "        save_best_only=True,\n",
        "        verbose=1,\n",
        "        mode='max'\n",
        "    ),\n",
        "    ModelCheckpoint(\n",
        "        'enhanced_model_auc.keras',\n",
        "        monitor='val_auc',\n",
        "        save_best_only=True,\n",
        "        verbose=1,\n",
        "        mode='max'\n",
        "    )\n",
        "]\n",
        "\n",
        "print(f\"‚úÖ Enhanced callbacks configured:\")\n",
        "print(f\"   üìà Increased early stopping patience: 25 epochs\")\n",
        "print(f\"   üîÑ Increased LR reduction patience: 15 epochs\")\n",
        "print(f\"   üíæ Dual checkpointing maintained\")\n",
        "\n",
        "# Model summary is printed inside the loop after building the model for each fold.\n",
        "# enhanced_model.summary()\n",
        "\n",
        "print(\"\\n‚úÖ ENHANCED MODEL ARCHITECTURE READY!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "AJ-jpcYV0OhI"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "import os, io, json, random\n",
        "import numpy as np, pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers as L, models, regularizers, callbacks\n",
        "import tensorflow.keras.backend as K\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from sklearn.metrics import (\n",
        "    roc_auc_score, matthews_corrcoef, accuracy_score,\n",
        "    precision_score, recall_score, f1_score, confusion_matrix\n",
        ")\n",
        "\n",
        "# ----------------- REPRODUCIBILITY -----------------\n",
        "SEED = 42\n",
        "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "# ----------------- FINAL PARAMETERS -----------------\n",
        "USE_VOL_ADJUSTED = True\n",
        "LABEL_THRESHOLD = 0.005  # 0.5%\n",
        "N_SPLITS = 5\n",
        "PURGE_SAMPLES = 60\n",
        "SEQ_LEN = 45\n",
        "TOP_K_FEATURES = 30\n",
        "N_ENSEMBLE = 3\n",
        "MIN_TRADE_MCC = 0.15\n",
        "MIN_TRADE_ROC = 0.60\n",
        "\n",
        "os.makedirs(\"artifacts_final\", exist_ok=True)\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"LSTM-CNN v10 FINAL: RAW ENSEMBLE (NO CALIBRATION)\")\n",
        "print(\"=\"*70)\n",
        "print(f\"‚úÖ External features integrated\")\n",
        "print(f\"‚úÖ Volatility-adjusted labeling (0.5% threshold)\")\n",
        "print(f\"‚úÖ Raw ensemble predictions (NO isotonic calibration)\")\n",
        "print(f\"‚úÖ Top features: {TOP_K_FEATURES}\")\n",
        "print(f\"‚úÖ Min tradeable: MCC {MIN_TRADE_MCC}, ROC {MIN_TRADE_ROC}\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "# ----------------- FOCAL LOSS -----------------\n",
        "def focal_loss(gamma=2.0, alpha=0.25):\n",
        "    def focal_loss_fixed(y_true, y_pred):\n",
        "        y_true = K.cast(y_true, tf.float32)\n",
        "        y_pred = K.clip(y_pred, K.epsilon(), 1.0 - K.epsilon())\n",
        "\n",
        "        ce = -y_true * K.log(y_pred)\n",
        "        weight = alpha * y_true * K.pow(1 - y_pred, gamma)\n",
        "        focal_loss_value = weight * ce\n",
        "\n",
        "        ce_neg = -(1 - y_true) * K.log(1 - y_pred)\n",
        "        weight_neg = (1 - alpha) * (1 - y_true) * K.pow(y_pred, gamma)\n",
        "        focal_loss_value += weight_neg * ce_neg\n",
        "\n",
        "        return K.mean(focal_loss_value)\n",
        "\n",
        "    return focal_loss_fixed\n",
        "\n",
        "# ----------------- BALANCED BATCH GENERATOR -----------------\n",
        "class BalancedBatchGenerator(tf.keras.utils.Sequence):\n",
        "    def __init__(self, X, y, batch_size=16, shuffle=True):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "\n",
        "        self.idx_0 = np.where(y == 0)[0]\n",
        "        self.idx_1 = np.where(y == 1)[0]\n",
        "\n",
        "        self.samples_per_class = min(len(self.idx_0), len(self.idx_1))\n",
        "        self.batches_per_class = self.batch_size // 2\n",
        "        self.n_batches = max(1, self.samples_per_class // self.batches_per_class)\n",
        "\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n_batches\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(self.idx_0)\n",
        "            np.random.shuffle(self.idx_1)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        start = idx * self.batches_per_class\n",
        "        end = min(start + self.batches_per_class, self.samples_per_class)\n",
        "\n",
        "        batch_idx_0 = self.idx_0[start:end]\n",
        "        batch_idx_1 = self.idx_1[start:end]\n",
        "\n",
        "        if len(batch_idx_0) < self.batches_per_class:\n",
        "            batch_idx_0 = np.pad(batch_idx_0, (0, self.batches_per_class - len(batch_idx_0)), mode='wrap')\n",
        "        if len(batch_idx_1) < self.batches_per_class:\n",
        "            batch_idx_1 = np.pad(batch_idx_1, (0, self.batches_per_class - len(batch_idx_1)), mode='wrap')\n",
        "\n",
        "        batch_idx = np.concatenate([batch_idx_0, batch_idx_1])\n",
        "        np.random.shuffle(batch_idx)\n",
        "\n",
        "        return self.X[batch_idx], self.y[batch_idx]\n",
        "\n",
        "# ----------------- MODEL -----------------\n",
        "def build_lstm_cnn_final(input_shape):\n",
        "    \"\"\"Final optimized LSTM-CNN: ~70k params\"\"\"\n",
        "    inp = L.Input(shape=input_shape)\n",
        "\n",
        "    x = L.LayerNormalization()(inp)\n",
        "\n",
        "    # CNN block\n",
        "    x = L.Conv1D(48, 3, padding='same',\n",
        "                 kernel_regularizer=regularizers.l2(1e-3))(x)\n",
        "    x = L.BatchNormalization()(x)\n",
        "    x = L.ReLU()(x)\n",
        "    x = L.SpatialDropout1D(0.2)(x)\n",
        "\n",
        "    # Bidirectional LSTM\n",
        "    x = L.Bidirectional(\n",
        "        L.LSTM(64, dropout=0.2, recurrent_dropout=0.15,\n",
        "               kernel_regularizer=regularizers.l2(1e-3))\n",
        "    )(x)\n",
        "\n",
        "    # Dense layers\n",
        "    x = L.Dense(48, activation='relu',\n",
        "                kernel_regularizer=regularizers.l2(1e-3))(x)\n",
        "    x = L.BatchNormalization()(x)\n",
        "    x = L.Dropout(0.3)(x)\n",
        "\n",
        "    x = L.Dense(24, activation='relu',\n",
        "                kernel_regularizer=regularizers.l2(1e-3))(x)\n",
        "    x = L.Dropout(0.3)(x)\n",
        "\n",
        "    out = L.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    model = models.Model(inp, out)\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(5e-4, clipnorm=1.0),\n",
        "        loss=focal_loss(gamma=2.0, alpha=0.25),\n",
        "        metrics=['accuracy', tf.keras.metrics.AUC(name='AUC')]\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# ----------------- THRESHOLD SEARCH -----------------\n",
        "def pick_best_threshold(y_true, y_prob):\n",
        "    \"\"\"Find optimal threshold by MCC\"\"\"\n",
        "    best_thr, best_mcc = 0.5, -1\n",
        "    for thr in np.linspace(0.40, 0.60, 21):\n",
        "        y_pred = (y_prob >= thr).astype(int)\n",
        "        mcc = matthews_corrcoef(y_true, y_pred)\n",
        "        if mcc > best_mcc:\n",
        "            best_mcc, best_thr = mcc, thr\n",
        "    return best_thr, best_mcc\n",
        "\n",
        "# ----------------- DATA CLEANING -----------------\n",
        "def clean_data(df):\n",
        "    \"\"\"Robust data cleaning\"\"\"\n",
        "    df = df.replace([np.inf, -np.inf], np.nan)\n",
        "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "\n",
        "    # Clip extremes\n",
        "    for col in numeric_cols:\n",
        "        if df[col].dtype in [np.float64, np.float32]:\n",
        "            lower = df[col].quantile(0.001)\n",
        "            upper = df[col].quantile(0.999)\n",
        "            df[col] = df[col].clip(lower, upper)\n",
        "\n",
        "    # Fill NaN\n",
        "    df[numeric_cols] = df[numeric_cols].fillna(method='ffill').fillna(method='bfill')\n",
        "    df = df.dropna()\n",
        "\n",
        "    return df\n",
        "\n",
        "# ----------------- DATA PREP -----------------\n",
        "def prepare_final_dataset():\n",
        "    assert 'nifty_data' in globals(), \"‚ùå Missing 'nifty_data'\"\n",
        "    print(\"üìä Preparing final dataset...\\n\")\n",
        "\n",
        "    df = nifty_data.copy()\n",
        "    df = clean_data(df)\n",
        "\n",
        "    df = df.dropna(subset=['Close'])\n",
        "    df['Next_Return'] = df['Close'].pct_change().shift(-1)\n",
        "\n",
        "    if USE_VOL_ADJUSTED and 'ATR_14' in df.columns:\n",
        "        print(\"  ‚úì ATR-adjusted labeling\")\n",
        "        df['ATR_Pct'] = df['ATR_14'] / (df['Close'] + 1e-8)\n",
        "        df['Vol_Adj_Move'] = df['Next_Return'] / (df['ATR_Pct'] + 1e-6)\n",
        "        df['Vol_Adj_Move'] = df['Vol_Adj_Move'].clip(-10, 10)\n",
        "\n",
        "        threshold_vol_adj = 0.3\n",
        "        df['label'] = np.where(df['Vol_Adj_Move'] > threshold_vol_adj, 1,\n",
        "                       np.where(df['Vol_Adj_Move'] < -threshold_vol_adj, 0, np.nan))\n",
        "    else:\n",
        "        print(f\"  ‚úì {LABEL_THRESHOLD*100:.1f}% threshold labeling\")\n",
        "        df['label'] = np.where(df['Next_Return'] > LABEL_THRESHOLD, 1,\n",
        "                       np.where(df['Next_Return'] < -LABEL_THRESHOLD, 0, np.nan))\n",
        "\n",
        "    df = df.dropna(subset=['label']).copy()\n",
        "    y_full = df['label'].astype(int).values\n",
        "    dates = pd.to_datetime(df['Date']).values\n",
        "\n",
        "    # Feature selection\n",
        "    exclude = ['Date','Symbol','Open','High','Low','Close','Volume',\n",
        "               'Next_Return','label','ATR_Pct','Vol_Adj_Move',\n",
        "               'INDIAVIX','USDINR','OIL','SP500','HSI','US10Y',\n",
        "               'SMA_50','SMA_200']\n",
        "\n",
        "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "    features = [c for c in numeric_cols if c not in exclude]\n",
        "\n",
        "    X_full = df[features].values\n",
        "    X_full = np.nan_to_num(X_full, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "    up_pct = np.mean(y_full == 1)\n",
        "    print(f\"‚úÖ Dataset: X={X_full.shape}, y={y_full.shape}\")\n",
        "    print(f\"   UP: {np.sum(y_full==1)} ({up_pct*100:.1f}%), \"\n",
        "          f\"DOWN: {np.sum(y_full==0)} ({(1-up_pct)*100:.1f}%)\\n\")\n",
        "\n",
        "    return X_full, y_full, dates, features\n",
        "\n",
        "# ----------------- SPLITS -----------------\n",
        "def purged_time_series_splits(dates, n_splits=5, purge=60):\n",
        "    n = len(dates)\n",
        "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
        "    splits = []\n",
        "    for train_idx, test_idx in tscv.split(np.arange(n)):\n",
        "        if purge > 0 and len(train_idx) > purge:\n",
        "            train_idx = train_idx[:-purge]\n",
        "        if len(test_idx) > purge//2:\n",
        "            test_idx = test_idx[purge//2:]\n",
        "        splits.append((train_idx, test_idx))\n",
        "    return splits\n",
        "\n",
        "# ----------------- CREATE SEQUENCES -----------------\n",
        "def create_sequences(X, y, dates, seq_len):\n",
        "    Xs, ys, ds = [], [], []\n",
        "    for i in range(len(X) - seq_len):\n",
        "        Xs.append(X[i:i+seq_len])\n",
        "        ys.append(y[i+seq_len])\n",
        "        ds.append(dates[i+seq_len])\n",
        "    return np.array(Xs), np.array(ys), np.array(ds)\n",
        "\n",
        "# ============================================================\n",
        "# MAIN WALK-FORWARD VALIDATION\n",
        "# ============================================================\n",
        "\n",
        "X_full, y_full, dates_all, feature_names = prepare_final_dataset()\n",
        "splits = purged_time_series_splits(dates_all, n_splits=N_SPLITS, purge=PURGE_SAMPLES)\n",
        "\n",
        "# Storage\n",
        "oof_predictions_raw = []  # RAW ensemble predictions\n",
        "oof_true_labels = []\n",
        "fold_summaries = []\n",
        "all_selected_features = []\n",
        "\n",
        "print(f\"{'='*70}\")\n",
        "print(f\"WALK-FORWARD VALIDATION: {N_SPLITS} FOLDS\")\n",
        "print(f\"{'='*70}\\n\")\n",
        "\n",
        "for fold, (tr_idx, te_idx) in enumerate(splits):\n",
        "    print(f\"‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê FOLD {fold+1}/{N_SPLITS} ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\")\n",
        "\n",
        "    X_tr_base, X_val_base = X_full[tr_idx], X_full[te_idx]\n",
        "    y_tr_base, y_val_base = y_full[tr_idx], y_full[te_idx]\n",
        "    dates_tr_base, dates_val_base = dates_all[tr_idx], dates_all[te_idx]\n",
        "\n",
        "    # Safety cleaning\n",
        "    X_tr_base = np.nan_to_num(X_tr_base, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "    X_val_base = np.nan_to_num(X_val_base, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "    # Scaling\n",
        "    scaler = RobustScaler().fit(X_tr_base)\n",
        "    X_tr_s = scaler.transform(X_tr_base)\n",
        "    X_val_s = scaler.transform(X_val_base)\n",
        "\n",
        "    X_tr_s = np.nan_to_num(X_tr_s, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "    X_val_s = np.nan_to_num(X_val_s, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "    # Feature selection\n",
        "    selector = SelectKBest(mutual_info_classif, k=min(TOP_K_FEATURES, X_tr_s.shape[1]))\n",
        "    X_tr_sel = selector.fit_transform(X_tr_s, y_tr_base)\n",
        "    X_val_sel = selector.transform(X_val_s)\n",
        "\n",
        "    X_tr_sel = np.nan_to_num(X_tr_sel, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "    X_val_sel = np.nan_to_num(X_val_sel, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "    # Track selected features\n",
        "    if fold == 0:\n",
        "        selected_mask = selector.get_support()\n",
        "        selected_features = [feature_names[i] for i in range(len(feature_names)) if selected_mask[i]]\n",
        "        all_selected_features = selected_features\n",
        "        print(f\"‚îÇ Top 5 features: {', '.join(selected_features[:5])}\")\n",
        "\n",
        "    # Sequences\n",
        "    X_tr_seq, y_tr_seq, _ = create_sequences(X_tr_sel, y_tr_base, dates_tr_base, SEQ_LEN)\n",
        "    X_val_seq, y_val_seq, dates_val_seq = create_sequences(X_val_sel, y_val_base, dates_val_base, SEQ_LEN)\n",
        "\n",
        "    print(f\"‚îÇ Train: {X_tr_seq.shape[0]} seq | Val: {X_val_seq.shape[0]} seq\")\n",
        "\n",
        "    if len(X_tr_seq) < 50 or len(X_val_seq) < 20:\n",
        "        print(f\"‚îÇ ‚ö†Ô∏è  SKIP: Insufficient data\")\n",
        "        print(f\"‚ïö{'‚ïê'*40}‚ïù\\n\")\n",
        "        continue\n",
        "\n",
        "    print(f\"‚îÇ Train UP: {np.mean(y_tr_seq):.1%} | Val UP: {np.mean(y_val_seq):.1%}\")\n",
        "\n",
        "    # ENSEMBLE\n",
        "    print(f\"‚îÇ Training {N_ENSEMBLE} models...\")\n",
        "    ensemble_preds = []\n",
        "\n",
        "    for seed_offset in range(N_ENSEMBLE):\n",
        "        tf.random.set_seed(SEED + seed_offset)\n",
        "        np.random.seed(SEED + seed_offset)\n",
        "\n",
        "        model = build_lstm_cnn_final((SEQ_LEN, X_tr_seq.shape[2]))\n",
        "\n",
        "        if fold == 0 and seed_offset == 0:\n",
        "            print(f\"‚îÇ Model: {model.count_params():,} params\")\n",
        "\n",
        "        train_gen = BalancedBatchGenerator(X_tr_seq, y_tr_seq, batch_size=16, shuffle=True)\n",
        "        val_data = (X_val_seq, y_val_seq)\n",
        "\n",
        "        cb = [\n",
        "            callbacks.EarlyStopping(monitor='val_AUC', patience=25,\n",
        "                                   restore_best_weights=True, mode='max', verbose=0),\n",
        "            callbacks.ReduceLROnPlateau(monitor='val_AUC', factor=0.5, patience=12,\n",
        "                                       min_lr=1e-6, verbose=0, mode='max')\n",
        "        ]\n",
        "\n",
        "        model.fit(train_gen, validation_data=val_data, epochs=120, verbose=0, callbacks=cb)\n",
        "\n",
        "        pred = model.predict(X_val_seq, verbose=0).ravel()\n",
        "        ensemble_preds.append(pred)\n",
        "\n",
        "    # RAW ensemble average (NO CALIBRATION)\n",
        "    p_val_raw = np.mean(ensemble_preds, axis=0)\n",
        "\n",
        "    # Find best threshold\n",
        "    thr, mcc_thr = pick_best_threshold(y_val_seq, p_val_raw)\n",
        "    y_pred = (p_val_raw >= thr).astype(int)\n",
        "\n",
        "    # Metrics\n",
        "    mcc = matthews_corrcoef(y_val_seq, y_pred)\n",
        "    acc = accuracy_score(y_val_seq, y_pred)\n",
        "    prec = precision_score(y_val_seq, y_pred, zero_division=0)\n",
        "    rec = recall_score(y_val_seq, y_pred, zero_division=0)\n",
        "    f1 = f1_score(y_val_seq, y_pred, zero_division=0)\n",
        "\n",
        "    try:\n",
        "        roc = roc_auc_score(y_val_seq, p_val_raw)\n",
        "    except:\n",
        "        roc = 0.5\n",
        "\n",
        "    pred_up_pct = np.mean(y_pred == 1)\n",
        "\n",
        "    print(f\"‚îÇ\")\n",
        "    print(f\"‚îÇ üìä RESULTS (RAW ENSEMBLE):\")\n",
        "    print(f\"‚îÇ   Threshold: {thr:.3f}\")\n",
        "    print(f\"‚îÇ   Pred UP: {pred_up_pct:.1%}\")\n",
        "    print(f\"‚îÇ   MCC: {mcc:.4f} {'‚úÖ' if mcc >= MIN_TRADE_MCC else '‚ùå'}\")\n",
        "    print(f\"‚îÇ   ROC-AUC: {roc:.4f} {'‚úÖ' if roc >= MIN_TRADE_ROC else '‚ùå'}\")\n",
        "    print(f\"‚îÇ   Acc: {acc:.3f} | Prec: {prec:.3f} | Rec: {rec:.3f} | F1: {f1:.3f}\")\n",
        "\n",
        "    status = \"TRADE ‚úÖ\" if (mcc >= MIN_TRADE_MCC and roc >= MIN_TRADE_ROC) else \"NO_TRADE ‚ùå\"\n",
        "    print(f\"‚îÇ   ‚Üí {status}\")\n",
        "    print(f\"‚ïö{'‚ïê'*40}‚ïù\\n\")\n",
        "\n",
        "    # Store RAW predictions\n",
        "    oof_predictions_raw.extend(p_val_raw.tolist())\n",
        "    oof_true_labels.extend(y_val_seq.tolist())\n",
        "\n",
        "    fold_summaries.append({\n",
        "        \"fold\": fold + 1,\n",
        "        \"status\": status,\n",
        "        \"mcc\": float(mcc),\n",
        "        \"roc_auc\": float(roc),\n",
        "        \"accuracy\": float(acc),\n",
        "        \"precision\": float(prec),\n",
        "        \"recall\": float(rec),\n",
        "        \"f1\": float(f1),\n",
        "        \"threshold\": float(thr),\n",
        "        \"pred_up_pct\": float(pred_up_pct),\n",
        "        \"n_val\": int(len(y_val_seq)),\n",
        "        \"date_range\": f\"{pd.to_datetime(dates_val_seq[0]).date()} to {pd.to_datetime(dates_val_seq[-1]).date()}\"\n",
        "    })\n",
        "\n",
        "\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"COMPUTING FINAL METRICS (RAW ENSEMBLE)\")\n",
        "print(f\"{'='*70}\\n\")\n",
        "\n",
        "oof_prob_raw = np.array(oof_predictions_raw)\n",
        "oof_true = np.array(oof_true_labels)\n",
        "\n",
        "# Find final threshold on RAW predictions\n",
        "thr_final, mcc_final = pick_best_threshold(oof_true, oof_prob_raw)\n",
        "y_pred_final = (oof_prob_raw >= thr_final).astype(int)\n",
        "\n",
        "# Final metrics\n",
        "acc_final = accuracy_score(oof_true, y_pred_final)\n",
        "prec_final = precision_score(oof_true, y_pred_final, zero_division=0)\n",
        "rec_final = recall_score(oof_true, y_pred_final, zero_division=0)\n",
        "f1_final = f1_score(oof_true, y_pred_final, zero_division=0)\n",
        "roc_final = roc_auc_score(oof_true, oof_prob_raw)\n",
        "\n",
        "pred_up_final = np.mean(y_pred_final == 1)\n",
        "\n",
        "print(f\"{'='*70}\")\n",
        "print(f\"FINAL OUT-OF-SAMPLE RESULTS ({len(oof_true)} samples)\")\n",
        "print(f\"{'='*70}\")\n",
        "print(f\"Threshold:   {thr_final:.3f}\")\n",
        "print(f\"Pred UP:     {pred_up_final:.1%}\")\n",
        "print(f\"\")\n",
        "print(f\"MCC:         {mcc_final:.4f}  {'üéØ EXCELLENT' if mcc_final > 0.20 else '‚úÖ GOOD' if mcc_final > 0.15 else '‚ö†Ô∏è MARGINAL'}\")\n",
        "print(f\"ROC-AUC:     {roc_final:.4f}  {'üéØ EXCELLENT' if roc_final > 0.65 else '‚úÖ GOOD' if roc_final > 0.60 else '‚ö†Ô∏è MARGINAL'}\")\n",
        "print(f\"\")\n",
        "print(f\"Accuracy:    {acc_final:.3f}\")\n",
        "print(f\"Precision:   {prec_final:.3f}\")\n",
        "print(f\"Recall:      {rec_final:.3f}\")\n",
        "print(f\"F1-Score:    {f1_final:.3f}\")\n",
        "print(f\"{'='*70}\\n\")\n",
        "\n",
        "# ============================================================\n",
        "# FOLD DIAGNOSTICS\n",
        "# ============================================================\n",
        "\n",
        "fold_df = pd.DataFrame(fold_summaries)\n",
        "tradeable_count = sum('TRADE ‚úÖ' in str(s['status']) for s in fold_summaries)\n",
        "\n",
        "print(\"üìã FOLD-BY-FOLD PERFORMANCE:\")\n",
        "print(fold_df[['fold', 'status', 'mcc', 'roc_auc', 'precision', 'recall', 'f1']])\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(f\"‚úÖ TRADEABLE FOLDS: {tradeable_count}/{len(fold_summaries)} ({tradeable_count/len(fold_summaries)*100:.0f}%)\")\n",
        "print(f\"{'='*70}\\n\")\n",
        "\n",
        "# ============================================================\n",
        "# SAVE ARTIFACTS\n",
        "# ============================================================\n",
        "\n",
        "fold_df.to_csv(\"artifacts_final/fold_results_v10.csv\", index=False)\n",
        "fold_df.to_json(\"artifacts_final/fold_results_v10.json\", orient='records', indent=2)\n",
        "\n",
        "np.save(\"artifacts_final/oof_predictions_raw_v10.npy\", oof_prob_raw)\n",
        "np.save(\"artifacts_final/oof_true_labels_v10.npy\", oof_true)\n",
        "np.save(\"artifacts_final/oof_predictions_final_v10.npy\", y_pred_final)\n",
        "\n",
        "# Save summary\n",
        "summary = {\n",
        "    \"version\": \"v10_final\",\n",
        "    \"configuration\": {\n",
        "        \"no_calibration\": True,\n",
        "        \"raw_ensemble_only\": True,\n",
        "        \"label_threshold\": f\"{LABEL_THRESHOLD*100}%\",\n",
        "        \"top_features\": TOP_K_FEATURES,\n",
        "        \"ensemble_size\": N_ENSEMBLE,\n",
        "        \"sequence_length\": SEQ_LEN\n",
        "    },\n",
        "    \"final_metrics\": {\n",
        "        \"mcc\": float(mcc_final),\n",
        "        \"roc_auc\": float(roc_final),\n",
        "        \"accuracy\": float(acc_final),\n",
        "        \"precision\": float(prec_final),\n",
        "        \"recall\": float(rec_final),\n",
        "        \"f1\": float(f1_final),\n",
        "        \"threshold\": float(thr_final)\n",
        "    },\n",
        "    \"fold_summary\": {\n",
        "        \"total_folds\": len(fold_summaries),\n",
        "        \"tradeable_folds\": tradeable_count,\n",
        "        \"tradeable_percentage\": float(tradeable_count/len(fold_summaries)*100)\n",
        "    },\n",
        "    \"top_features\": all_selected_features[:10],\n",
        "    \"is_tradeable\": bool(mcc_final > 0.15 and roc_final > 0.60)\n",
        "}\n",
        "\n",
        "with open(\"artifacts_final/final_summary_v10.json\", \"w\") as f:\n",
        "    json.dump(summary, f, indent=2)\n",
        "\n",
        "\n",
        "\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "# 1. Fold MCCs\n",
        "ax1 = axes[0, 0]\n",
        "fold_mccs = [f['mcc'] for f in fold_summaries]\n",
        "colors = ['green' if 'TRADE ‚úÖ' in str(f['status']) else 'red' for f in fold_summaries]\n",
        "ax1.bar(range(1, len(fold_mccs)+1), fold_mccs, color=colors, alpha=0.7, edgecolor='black')\n",
        "ax1.axhline(y=MIN_TRADE_MCC, color='orange', linestyle='--', linewidth=2, label=f'Min ({MIN_TRADE_MCC})')\n",
        "ax1.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
        "ax1.set_xlabel('Fold', fontweight='bold', fontsize=11)\n",
        "ax1.set_ylabel('MCC', fontweight='bold', fontsize=11)\n",
        "ax1.set_title('Fold-wise MCC (Green=Tradeable)', fontweight='bold', fontsize=12)\n",
        "ax1.legend()\n",
        "ax1.grid(alpha=0.3)\n",
        "\n",
        "# 2. ROC-AUC\n",
        "ax2 = axes[0, 1]\n",
        "fold_rocs = [f['roc_auc'] for f in fold_summaries]\n",
        "ax2.plot(range(1, len(fold_rocs)+1), fold_rocs, 'o-', linewidth=2, markersize=8, color='seagreen')\n",
        "ax2.axhline(y=MIN_TRADE_ROC, color='orange', linestyle='--', linewidth=2, label=f'Target ({MIN_TRADE_ROC})')\n",
        "ax2.axhline(y=0.5, color='gray', linestyle='-', alpha=0.3, label='Random')\n",
        "ax2.fill_between(range(1, len(fold_rocs)+1), 0.5, MIN_TRADE_ROC, alpha=0.1, color='red')\n",
        "ax2.fill_between(range(1, len(fold_rocs)+1), MIN_TRADE_ROC, 1.0, alpha=0.1, color='green')\n",
        "ax2.set_xlabel('Fold', fontweight='bold', fontsize=11)\n",
        "ax2.set_ylabel('ROC-AUC', fontweight='bold', fontsize=11)\n",
        "ax2.set_title('Fold-wise ROC-AUC', fontweight='bold', fontsize=12)\n",
        "ax2.legend()\n",
        "ax2.grid(alpha=0.3)\n",
        "\n",
        "# 3. Precision vs Recall\n",
        "ax3 = axes[0, 2]\n",
        "fold_precs = [f['precision'] for f in fold_summaries]\n",
        "fold_recs = [f['recall'] for f in fold_summaries]\n",
        "ax3.scatter(fold_recs, fold_precs, s=100, alpha=0.7, c=colors, edgecolor='black')\n",
        "for i, fold in enumerate(fold_summaries):\n",
        "    ax3.annotate(f\"F{fold['fold']}\", (fold_recs[i], fold_precs[i]), fontsize=9, ha='center')\n",
        "ax3.set_xlabel('Recall', fontweight='bold', fontsize=11)\n",
        "ax3.set_ylabel('Precision', fontweight='bold', fontsize=11)\n",
        "ax3.set_title('Precision-Recall Trade-off', fontweight='bold', fontsize=12)\n",
        "ax3.grid(alpha=0.3)\n",
        "\n",
        "# 4. Prediction distribution\n",
        "ax4 = axes[1, 0]\n",
        "ax4.hist(oof_prob_raw, bins=40, alpha=0.7, edgecolor='black', color='steelblue')\n",
        "ax4.axvline(x=thr_final, color='red', linestyle='--', linewidth=2, label=f'Threshold ({thr_final:.3f})')\n",
        "ax4.set_xlabel('Raw Ensemble Probability', fontweight='bold', fontsize=11)\n",
        "ax4.set_ylabel('Frequency', fontweight='bold', fontsize=11)\n",
        "ax4.set_title('OOF Prediction Distribution', fontweight='bold', fontsize=12)\n",
        "ax4.legend()\n",
        "ax4.grid(alpha=0.3)\n",
        "\n",
        "# 5. Confusion matrix\n",
        "ax5 = axes[1, 1]\n",
        "cm = confusion_matrix(oof_true, y_pred_final)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, ax=ax5,\n",
        "            xticklabels=['DOWN', 'UP'], yticklabels=['DOWN', 'UP'])\n",
        "ax5.set_xlabel('Predicted', fontweight='bold', fontsize=11)\n",
        "ax5.set_ylabel('Actual', fontweight='bold', fontsize=11)\n",
        "ax5.set_title('Confusion Matrix (OOF)', fontweight='bold', fontsize=12)\n",
        "\n",
        "# 6. Metrics summary\n",
        "ax6 = axes[1, 2]\n",
        "ax6.axis('off')\n",
        "metrics_text = f\"\"\"\n",
        "FINAL METRICS SUMMARY\n",
        "\n",
        "Out-of-Sample: {len(oof_true)} samples\n",
        "\n",
        "MCC:        {mcc_final:.4f}\n",
        "ROC-AUC:    {roc_final:.4f}\n",
        "Accuracy:   {acc_final:.3f}\n",
        "Precision:  {prec_final:.3f}\n",
        "Recall:     {rec_final:.3f}\n",
        "F1-Score:   {f1_final:.3f}\n",
        "\n",
        "Tradeable Folds: {tradeable_count}/{len(fold_summaries)}\n",
        "\n",
        "Status: {'‚úÖ TRADEABLE' if mcc_final > 0.15 and roc_final > 0.60 else '‚ö†Ô∏è MARGINAL'}\n",
        "\"\"\"\n",
        "ax6.text(0.1, 0.5, metrics_text, fontsize=11, family='monospace',\n",
        "         verticalalignment='center', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.3))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"artifacts_final/final_results_v10.png\", dpi=150, bbox_inches='tight')\n",
        "plt.close()\n",
        "\n",
        "print(f\"üíæ All artifacts saved to ./artifacts_final/\")\n",
        "print(f\"   - fold_results_v10.csv/json\")\n",
        "print(f\"   - final_summary_v10.json\")\n",
        "print(f\"   - OOF predictions (.npy)\")\n",
        "print(f\"   - final_results_v10.png\\n\")\n",
        "\n",
        "print(f\"{'='*70}\")\n",
        "print(\"üéØ V10 FINAL COMPLETE (NO CALIBRATION)\")\n",
        "print(f\"{'='*70}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jZnSniSkliaI"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "import os, json, joblib\n",
        "import numpy as np, pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers as L, models, regularizers, callbacks\n",
        "import tensorflow.keras.backend as K\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Reproducibility\n",
        "SEED = 42\n",
        "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"PRODUCTION MODEL TRAINING (BEST FOLD CONFIGURATION)\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "\n",
        "\n",
        "# Load fold results\n",
        "fold_df = pd.read_json(\"artifacts_final/fold_results_v10.json\")\n",
        "\n",
        "# Best fold = highest MCC among tradeable folds\n",
        "tradeable_folds = fold_df[fold_df['status'] == 'TRADE ‚úÖ']\n",
        "\n",
        "if len(tradeable_folds) == 0:\n",
        "    print(\"‚ö†Ô∏è No tradeable folds found. Using best MCC fold instead.\")\n",
        "    best_fold_idx = fold_df['mcc'].idxmax()\n",
        "else:\n",
        "    best_fold_idx = tradeable_folds['mcc'].idxmax()\n",
        "\n",
        "best_fold = fold_df.loc[best_fold_idx]\n",
        "\n",
        "print(f\"üéØ BEST FOLD IDENTIFIED: Fold {best_fold['fold']}\")\n",
        "print(f\"   MCC: {best_fold['mcc']:.4f}\")\n",
        "print(f\"   ROC-AUC: {best_fold['roc_auc']:.4f}\")\n",
        "print(f\"   Precision: {best_fold['precision']:.3f}\")\n",
        "print(f\"   Recall: {best_fold['recall']:.3f}\")\n",
        "print(f\"   Threshold: {best_fold['threshold']:.3f}\")\n",
        "print(f\"   Status: {best_fold['status']}\\n\")\n",
        "\n",
        "\n",
        "\n",
        "SEQ_LEN = 45\n",
        "TOP_K_FEATURES = 30\n",
        "N_ENSEMBLE = 3\n",
        "BEST_THRESHOLD = best_fold['threshold']\n",
        "\n",
        "print(\"üìã Production Configuration:\")\n",
        "print(f\"   Sequence Length: {SEQ_LEN}\")\n",
        "print(f\"   Top Features: {TOP_K_FEATURES}\")\n",
        "print(f\"   Ensemble Size: {N_ENSEMBLE}\")\n",
        "print(f\"   Decision Threshold: {BEST_THRESHOLD:.3f}\\n\")\n",
        "\n",
        "\n",
        "\n",
        "def focal_loss(gamma=2.0, alpha=0.25):\n",
        "    def focal_loss_fixed(y_true, y_pred):\n",
        "        y_true = K.cast(y_true, tf.float32)\n",
        "        y_pred = K.clip(y_pred, K.epsilon(), 1.0 - K.epsilon())\n",
        "\n",
        "        ce = -y_true * K.log(y_pred)\n",
        "        weight = alpha * y_true * K.pow(1 - y_pred, gamma)\n",
        "        focal_loss_value = weight * ce\n",
        "\n",
        "        ce_neg = -(1 - y_true) * K.log(1 - y_pred)\n",
        "        weight_neg = (1 - alpha) * (1 - y_true) * K.pow(y_pred, gamma)\n",
        "        focal_loss_value += weight_neg * ce_neg\n",
        "\n",
        "        return K.mean(focal_loss_value)\n",
        "\n",
        "    return focal_loss_fixed\n",
        "\n",
        "class BalancedBatchGenerator(tf.keras.utils.Sequence):\n",
        "    def __init__(self, X, y, batch_size=16, shuffle=True):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "\n",
        "        self.idx_0 = np.where(y == 0)[0]\n",
        "        self.idx_1 = np.where(y == 1)[0]\n",
        "\n",
        "        self.samples_per_class = min(len(self.idx_0), len(self.idx_1))\n",
        "        self.batches_per_class = self.batch_size // 2\n",
        "        self.n_batches = max(1, self.samples_per_class // self.batches_per_class)\n",
        "\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n_batches\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(self.idx_0)\n",
        "            np.random.shuffle(self.idx_1)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        start = idx * self.batches_per_class\n",
        "        end = min(start + self.batches_per_class, self.samples_per_class)\n",
        "\n",
        "        batch_idx_0 = self.idx_0[start:end]\n",
        "        batch_idx_1 = self.idx_1[start:end]\n",
        "\n",
        "        if len(batch_idx_0) < self.batches_per_class:\n",
        "            batch_idx_0 = np.pad(batch_idx_0, (0, self.batches_per_class - len(batch_idx_0)), mode='wrap')\n",
        "        if len(batch_idx_1) < self.batches_per_class:\n",
        "            batch_idx_1 = np.pad(batch_idx_1, (0, self.batches_per_class - len(batch_idx_1)), mode='wrap')\n",
        "\n",
        "        batch_idx = np.concatenate([batch_idx_0, batch_idx_1])\n",
        "        np.random.shuffle(batch_idx)\n",
        "\n",
        "        return self.X[batch_idx], self.y[batch_idx]\n",
        "\n",
        "def build_production_model(input_shape):\n",
        "    \"\"\"Production LSTM-CNN model\"\"\"\n",
        "    inp = L.Input(shape=input_shape)\n",
        "\n",
        "    x = L.LayerNormalization()(inp)\n",
        "\n",
        "    x = L.Conv1D(48, 3, padding='same',\n",
        "                 kernel_regularizer=regularizers.l2(1e-3))(x)\n",
        "    x = L.BatchNormalization()(x)\n",
        "    x = L.ReLU()(x)\n",
        "    x = L.SpatialDropout1D(0.2)(x)\n",
        "\n",
        "    x = L.Bidirectional(\n",
        "        L.LSTM(64, dropout=0.2, recurrent_dropout=0.15,\n",
        "               kernel_regularizer=regularizers.l2(1e-3))\n",
        "    )(x)\n",
        "\n",
        "    x = L.Dense(48, activation='relu',\n",
        "                kernel_regularizer=regularizers.l2(1e-3))(x)\n",
        "    x = L.BatchNormalization()(x)\n",
        "    x = L.Dropout(0.3)(x)\n",
        "\n",
        "    x = L.Dense(24, activation='relu',\n",
        "                kernel_regularizer=regularizers.l2(1e-3))(x)\n",
        "    x = L.Dropout(0.3)(x)\n",
        "\n",
        "    out = L.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    model = models.Model(inp, out)\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(5e-4, clipnorm=1.0),\n",
        "        loss=focal_loss(gamma=2.0, alpha=0.25),\n",
        "        metrics=['accuracy', tf.keras.metrics.AUC(name='AUC')]\n",
        "    )\n",
        "    return model\n",
        "\n",
        "def create_sequences(X, y, dates, seq_len):\n",
        "    Xs, ys, ds = [], [], []\n",
        "    for i in range(len(X) - seq_len):\n",
        "        Xs.append(X[i:i+seq_len])\n",
        "        ys.append(y[i+seq_len])\n",
        "        ds.append(dates[i+seq_len])\n",
        "    return np.array(Xs), np.array(ys), np.array(ds)\n",
        "\n",
        "\n",
        "\n",
        "print(\"üìä Preparing full dataset for production training...\\n\")\n",
        "\n",
        "# Reload and clean data\n",
        "df = nifty_data.copy()\n",
        "df = df.replace([np.inf, -np.inf], np.nan)\n",
        "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "for col in numeric_cols:\n",
        "    if df[col].dtype in [np.float64, np.float32]:\n",
        "        lower = df[col].quantile(0.001)\n",
        "        upper = df[col].quantile(0.999)\n",
        "        df[col] = df[col].clip(lower, upper)\n",
        "df[numeric_cols] = df[numeric_cols].fillna(method='ffill').fillna(method='bfill')\n",
        "df = df.dropna()\n",
        "\n",
        "# Labels\n",
        "df['Next_Return'] = df['Close'].pct_change().shift(-1)\n",
        "\n",
        "if 'ATR_14' in df.columns:\n",
        "    df['ATR_Pct'] = df['ATR_14'] / (df['Close'] + 1e-8)\n",
        "    df['Vol_Adj_Move'] = df['Next_Return'] / (df['ATR_Pct'] + 1e-6)\n",
        "    df['Vol_Adj_Move'] = df['Vol_Adj_Move'].clip(-10, 10)\n",
        "    df['label'] = np.where(df['Vol_Adj_Move'] > 0.3, 1,\n",
        "                   np.where(df['Vol_Adj_Move'] < -0.3, 0, np.nan))\n",
        "else:\n",
        "    df['label'] = np.where(df['Next_Return'] > 0.005, 1,\n",
        "                   np.where(df['Next_Return'] < -0.005, 0, np.nan))\n",
        "\n",
        "df = df.dropna(subset=['label']).copy()\n",
        "y_full = df['label'].astype(int).values\n",
        "dates = pd.to_datetime(df['Date']).values\n",
        "\n",
        "# Features\n",
        "exclude = ['Date','Symbol','Open','High','Low','Close','Volume',\n",
        "           'Next_Return','label','ATR_Pct','Vol_Adj_Move',\n",
        "           'INDIAVIX','USDINR','OIL','SP500','HSI','US10Y',\n",
        "           'SMA_50','SMA_200']\n",
        "\n",
        "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "features = [c for c in numeric_cols if c not in exclude]\n",
        "\n",
        "X_full = df[features].values\n",
        "X_full = np.nan_to_num(X_full, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "print(f\"‚úÖ Full dataset: {X_full.shape[0]} samples, {X_full.shape[1]} features\")\n",
        "print(f\"   UP: {np.sum(y_full==1)} ({np.mean(y_full==1)*100:.1f}%)\")\n",
        "print(f\"   DOWN: {np.sum(y_full==0)} ({np.mean(y_full==0)*100:.1f}%)\\n\")\n",
        "\n",
        "\n",
        "\n",
        "print(\"üèóÔ∏è Training production ensemble...\\n\")\n",
        "\n",
        "# Use last 80% for training, leave last 20% for validation\n",
        "train_size = int(len(X_full) * 0.8)\n",
        "X_train, X_test = X_full[:train_size], X_full[train_size:]\n",
        "y_train, y_test = y_full[:train_size], y_full[train_size:]\n",
        "dates_train, dates_test = dates[:train_size], dates[train_size:]\n",
        "\n",
        "# Scale\n",
        "scaler = RobustScaler().fit(X_train)\n",
        "X_train_s = scaler.transform(X_train)\n",
        "X_test_s = scaler.transform(X_test)\n",
        "\n",
        "X_train_s = np.nan_to_num(X_train_s, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "X_test_s = np.nan_to_num(X_test_s, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "# Feature selection\n",
        "selector = SelectKBest(mutual_info_classif, k=TOP_K_FEATURES)\n",
        "X_train_sel = selector.fit_transform(X_train_s, y_train)\n",
        "X_test_sel = selector.transform(X_test_s)\n",
        "\n",
        "X_train_sel = np.nan_to_num(X_train_sel, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "X_test_sel = np.nan_to_num(X_test_sel, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "# Get selected feature names\n",
        "selected_mask = selector.get_support()\n",
        "selected_features = [features[i] for i in range(len(features)) if selected_mask[i]]\n",
        "print(f\"üîç Top 10 Features: {', '.join(selected_features[:10])}\\n\")\n",
        "\n",
        "# Create sequences\n",
        "X_train_seq, y_train_seq, _ = create_sequences(X_train_sel, y_train, dates_train, SEQ_LEN)\n",
        "X_test_seq, y_test_seq, dates_test_seq = create_sequences(X_test_sel, y_test, dates_test, SEQ_LEN)\n",
        "\n",
        "print(f\"üì¶ Training sequences: {X_train_seq.shape[0]}\")\n",
        "print(f\"üì¶ Test sequences: {X_test_seq.shape[0]}\\n\")\n",
        "\n",
        "# Train ensemble\n",
        "production_models = []\n",
        "ensemble_preds_test = []\n",
        "\n",
        "for i in range(N_ENSEMBLE):\n",
        "    print(f\"Training model {i+1}/{N_ENSEMBLE}...\")\n",
        "    tf.random.set_seed(SEED + i)\n",
        "    np.random.seed(SEED + i)\n",
        "\n",
        "    model = build_production_model((SEQ_LEN, X_train_seq.shape[2]))\n",
        "\n",
        "    train_gen = BalancedBatchGenerator(X_train_seq, y_train_seq, batch_size=16, shuffle=True)\n",
        "    val_data = (X_test_seq, y_test_seq)\n",
        "\n",
        "    cb = [\n",
        "        callbacks.EarlyStopping(monitor='val_AUC', patience=30,\n",
        "                               restore_best_weights=True, mode='max', verbose=0),\n",
        "        callbacks.ReduceLROnPlateau(monitor='val_AUC', factor=0.5, patience=15,\n",
        "                                   min_lr=1e-6, verbose=0, mode='max')\n",
        "    ]\n",
        "\n",
        "    history = model.fit(train_gen, validation_data=val_data, epochs=150, verbose=0, callbacks=cb)\n",
        "\n",
        "    # Save model\n",
        "    model.save(f\"artifacts_final/production_model_{i}.keras\")\n",
        "    production_models.append(model)\n",
        "\n",
        "    # Test prediction\n",
        "    pred = model.predict(X_test_seq, verbose=0).ravel()\n",
        "    ensemble_preds_test.append(pred)\n",
        "\n",
        "    print(f\"  ‚úì Model {i+1} trained (best val AUC: {max(history.history['val_AUC']):.4f})\\n\")\n",
        "\n",
        "# Ensemble average\n",
        "test_pred_avg = np.mean(ensemble_preds_test, axis=0)\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.metrics import matthews_corrcoef, roc_auc_score, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "y_pred_test = (test_pred_avg >= BEST_THRESHOLD).astype(int)\n",
        "\n",
        "mcc_test = matthews_corrcoef(y_test_seq, y_pred_test)\n",
        "roc_test = roc_auc_score(y_test_seq, test_pred_avg)\n",
        "acc_test = accuracy_score(y_test_seq, y_pred_test)\n",
        "prec_test = precision_score(y_test_seq, y_pred_test, zero_division=0)\n",
        "rec_test = recall_score(y_test_seq, y_pred_test, zero_division=0)\n",
        "f1_test = f1_score(y_test_seq, y_pred_test, zero_division=0)\n",
        "\n",
        "print(f\"{'='*70}\")\n",
        "print(f\"PRODUCTION MODEL TEST PERFORMANCE\")\n",
        "print(f\"{'='*70}\")\n",
        "print(f\"Test samples: {len(y_test_seq)}\")\n",
        "print(f\"Threshold: {BEST_THRESHOLD:.3f}\")\n",
        "print(f\"\")\n",
        "print(f\"MCC:         {mcc_test:.4f}\")\n",
        "print(f\"ROC-AUC:     {roc_test:.4f}\")\n",
        "print(f\"Accuracy:    {acc_test:.3f}\")\n",
        "print(f\"Precision:   {prec_test:.3f}\")\n",
        "print(f\"Recall:      {rec_test:.3f}\")\n",
        "print(f\"F1-Score:    {f1_test:.3f}\")\n",
        "print(f\"{'='*70}\\n\")\n",
        "\n",
        "\n",
        "\n",
        "print(\"üîÆ GENERATING NEXT-DAY PREDICTION...\\n\")\n",
        "\n",
        "# Get last SEQ_LEN samples for prediction\n",
        "X_latest = X_test_sel[-SEQ_LEN:]\n",
        "X_latest_seq = X_latest.reshape(1, SEQ_LEN, -1)\n",
        "\n",
        "# Ensemble prediction\n",
        "latest_preds = []\n",
        "for model in production_models:\n",
        "    pred = model.predict(X_latest_seq, verbose=0).ravel()[0]\n",
        "    latest_preds.append(pred)\n",
        "\n",
        "latest_pred_avg = np.mean(latest_preds)\n",
        "latest_pred_std = np.std(latest_preds)\n",
        "latest_decision = \"UP üü¢\" if latest_pred_avg >= BEST_THRESHOLD else \"DOWN üî¥\"\n",
        "\n",
        "latest_date = pd.to_datetime(dates_test_seq[-1]).date()\n",
        "next_trading_day = latest_date + timedelta(days=1)\n",
        "\n",
        "print(f\"{'='*70}\")\n",
        "print(f\"NEXT-DAY PREDICTION FOR NIFTY 50\")\n",
        "print(f\"{'='*70}\")\n",
        "print(f\"Latest data date:  {latest_date}\")\n",
        "print(f\"Prediction for:    {next_trading_day}\")\n",
        "print(f\"\")\n",
        "print(f\"Probability (UP):  {latest_pred_avg:.4f} ¬± {latest_pred_std:.4f}\")\n",
        "print(f\"Threshold:         {BEST_THRESHOLD:.3f}\")\n",
        "print(f\"\")\n",
        "print(f\"üéØ PREDICTION:      {latest_decision}\")\n",
        "print(f\"\")\n",
        "print(f\"Confidence:\")\n",
        "if abs(latest_pred_avg - BEST_THRESHOLD) > 0.15:\n",
        "    print(f\"  ‚úÖ HIGH (strong signal)\")\n",
        "elif abs(latest_pred_avg - BEST_THRESHOLD) > 0.08:\n",
        "    print(f\"  ‚ö†Ô∏è  MODERATE\")\n",
        "else:\n",
        "    print(f\"  ‚ö†Ô∏è  LOW (near threshold)\")\n",
        "print(f\"{'='*70}\\n\")\n",
        "\n",
        "# ============================================================\n",
        "# 8. SAVE PRODUCTION ARTIFACTS\n",
        "# ============================================================\n",
        "\n",
        "# Save scaler and selector\n",
        "joblib.dump(scaler, \"artifacts_final/production_scaler.pkl\")\n",
        "joblib.dump(selector, \"artifacts_final/production_selector.pkl\")\n",
        "\n",
        "# Save metadata\n",
        "production_metadata = {\n",
        "    \"trained_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "    \"best_fold\": int(best_fold['fold']),\n",
        "    \"best_fold_mcc\": float(best_fold['mcc']),\n",
        "    \"best_fold_roc\": float(best_fold['roc_auc']),\n",
        "    \"threshold\": float(BEST_THRESHOLD),\n",
        "    \"sequence_length\": int(SEQ_LEN),\n",
        "    \"top_features\": int(TOP_K_FEATURES),\n",
        "    \"ensemble_size\": int(N_ENSEMBLE),\n",
        "    \"selected_features\": selected_features,\n",
        "    \"test_performance\": {\n",
        "        \"mcc\": float(mcc_test),\n",
        "        \"roc_auc\": float(roc_test),\n",
        "        \"accuracy\": float(acc_test),\n",
        "        \"precision\": float(prec_test),\n",
        "        \"recall\": float(rec_test),\n",
        "        \"f1\": float(f1_test)\n",
        "    },\n",
        "    \"next_day_prediction\": {\n",
        "        \"date\": str(next_trading_day),\n",
        "        \"probability_up\": float(latest_pred_avg),\n",
        "        \"std_dev\": float(latest_pred_std),\n",
        "        \"decision\": latest_decision\n",
        "    }\n",
        "}\n",
        "\n",
        "with open(\"artifacts_final/production_metadata.json\", \"w\") as f:\n",
        "    json.dump(production_metadata, f, indent=2)\n",
        "\n",
        "print(\"üíæ Production artifacts saved:\")\n",
        "print(\"   ‚úì 3 trained models (production_model_*.keras)\")\n",
        "print(\"   ‚úì Scaler (production_scaler.pkl)\")\n",
        "print(\"   ‚úì Feature selector (production_selector.pkl)\")\n",
        "print(\"   ‚úì Metadata (production_metadata.json)\\n\")\n",
        "\n",
        "# ============================================================\n",
        "# 9. VISUALIZATION\n",
        "# ============================================================\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "# Test predictions over time\n",
        "ax1 = axes[0, 0]\n",
        "test_dates_plot = [pd.to_datetime(d) for d in dates_test_seq]\n",
        "ax1.plot(test_dates_plot, test_pred_avg, label='Predicted Prob (UP)', color='blue', linewidth=1.5)\n",
        "ax1.axhline(y=BEST_THRESHOLD, color='red', linestyle='--', linewidth=2, label=f'Threshold ({BEST_THRESHOLD:.3f})')\n",
        "ax1.fill_between(test_dates_plot, 0, BEST_THRESHOLD, alpha=0.1, color='red', label='DOWN zone')\n",
        "ax1.fill_between(test_dates_plot, BEST_THRESHOLD, 1, alpha=0.1, color='green', label='UP zone')\n",
        "ax1.scatter(test_dates_plot, y_test_seq, c=y_test_seq, cmap='RdYlGn', alpha=0.3, s=10, label='Actual')\n",
        "ax1.set_xlabel('Date', fontweight='bold')\n",
        "ax1.set_ylabel('Probability', fontweight='bold')\n",
        "ax1.set_title('Production Model: Test Set Predictions', fontweight='bold')\n",
        "ax1.legend()\n",
        "ax1.grid(alpha=0.3)\n",
        "\n",
        "# Confusion matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "ax2 = axes[0, 1]\n",
        "cm = confusion_matrix(y_test_seq, y_pred_test)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, ax=ax2,\n",
        "            xticklabels=['DOWN', 'UP'], yticklabels=['DOWN', 'UP'])\n",
        "ax2.set_xlabel('Predicted', fontweight='bold')\n",
        "ax2.set_ylabel('Actual', fontweight='bold')\n",
        "ax2.set_title('Confusion Matrix (Test Set)', fontweight='bold')\n",
        "\n",
        "# Ensemble agreement\n",
        "ax3 = axes[1, 0]\n",
        "ensemble_std = np.std(ensemble_preds_test, axis=0)\n",
        "ax3.scatter(test_pred_avg, ensemble_std, c=y_test_seq, cmap='RdYlGn', alpha=0.5)\n",
        "ax3.axvline(x=BEST_THRESHOLD, color='red', linestyle='--', linewidth=2)\n",
        "ax3.set_xlabel('Ensemble Mean Probability', fontweight='bold')\n",
        "ax3.set_ylabel('Ensemble Std Dev', fontweight='bold')\n",
        "ax3.set_title('Ensemble Agreement Analysis', fontweight='bold')\n",
        "ax3.grid(alpha=0.3)\n",
        "\n",
        "# Feature importance (top 15)\n",
        "ax4 = axes[1, 1]\n",
        "feature_scores = selector.scores_[selected_mask]\n",
        "top_15_idx = np.argsort(feature_scores)[-15:]\n",
        "top_15_features = [selected_features[i] for i in range(len(selected_features)) if i in top_15_idx]\n",
        "top_15_scores = feature_scores[top_15_idx]\n",
        "\n",
        "ax4.barh(range(len(top_15_features)), top_15_scores, color='steelblue', edgecolor='black')\n",
        "ax4.set_yticks(range(len(top_15_features)))\n",
        "ax4.set_yticklabels(top_15_features, fontsize=8)\n",
        "ax4.set_xlabel('Mutual Information Score', fontweight='bold')\n",
        "ax4.set_title('Top 15 Feature Importance', fontweight='bold')\n",
        "ax4.grid(alpha=0.3, axis='x')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"artifacts_final/production_model_analysis.png\", dpi=150, bbox_inches='tight')\n",
        "plt.close()\n",
        "\n",
        "print(\"üìä Visualization saved: production_model_analysis.png\\n\")\n",
        "\n",
        "print(f\"{'='*70}\")\n",
        "print(\"‚úÖ PRODUCTION MODEL READY FOR DEPLOYMENT\")\n",
        "print(f\"{'='*70}\")\n",
        "print(f\"Next-day prediction: {latest_decision}\")\n",
        "print(f\"Use saved models for live predictions!\")\n",
        "print(f\"{'='*70}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "jaL9p6J-aLM5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import json\n",
        "from sklearn.metrics import confusion_matrix, roc_curve, auc, precision_recall_curve, matthews_corrcoef, accuracy_score\n",
        "\n",
        "os.makedirs(\"research_figures\", exist_ok=True)\n",
        "\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['font.family'] = 'serif'\n",
        "plt.rcParams['font.size'] = 12\n",
        "plt.rcParams['axes.labelsize'] = 14\n",
        "plt.rcParams['axes.titlesize'] = 16\n",
        "plt.rcParams['xtick.labelsize'] = 12\n",
        "plt.rcParams['ytick.labelsize'] = 12\n",
        "plt.rcParams['legend.fontsize'] = 12\n",
        "plt.rcParams['figure.dpi'] = 300\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"GENERATING PUBLICATION-QUALITY FIGURES\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "fold_df = pd.read_json(\"artifacts_final/fold_results_v10.json\")\n",
        "oof_prob = np.load(\"artifacts_final/oof_predictions_raw_v10.npy\")\n",
        "oof_true = np.load(\"artifacts_final/oof_true_labels_v10.npy\")\n",
        "\n",
        "with open(\"artifacts_final/production_metadata.json\", \"r\") as f:\n",
        "    prod_metadata = json.load(f)\n",
        "\n",
        "print(f\"‚úÖ Loaded data: {len(fold_df)} folds, {len(oof_prob)} samples\\n\")\n",
        "\n",
        "thresholds_test = np.linspace(0.35, 0.65, 31)\n",
        "mccs_test = []\n",
        "for thr in thresholds_test:\n",
        "    y_pred = (oof_prob >= thr).astype(int)\n",
        "    mcc = matthews_corrcoef(oof_true, y_pred)\n",
        "    mccs_test.append(mcc)\n",
        "best_idx = np.argmax(mccs_test)\n",
        "best_thr = thresholds_test[best_idx]\n",
        "best_mcc = mccs_test[best_idx]\n",
        "y_pred_final = (oof_prob >= best_thr).astype(int)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "folds = fold_df['fold'].values\n",
        "mccs = fold_df['mcc'].values\n",
        "colors = ['#2ecc71' if 'TRADE' in str(s) else '#95a5a6' for s in fold_df['status'].values]\n",
        "bars = ax.bar(folds, mccs, color=colors, alpha=0.85, edgecolor='black', linewidth=2, width=0.6)\n",
        "ax.axhline(y=0.15, color='#e74c3c', linestyle='--', linewidth=2.5, label='Tradeable Threshold (0.15)', zorder=5)\n",
        "ax.set_xlabel('Validation Fold', fontweight='bold', fontsize=14)\n",
        "ax.set_ylabel('Matthews Correlation Coefficient (MCC)', fontweight='bold', fontsize=14)\n",
        "ax.set_title('Model Performance Across Temporal Folds', fontweight='bold', fontsize=16, pad=20)\n",
        "ax.legend(loc='upper left', fontsize=12, framealpha=0.95)\n",
        "ax.set_ylim(0, max(mccs) * 1.15)\n",
        "ax.spines['top'].set_visible(False)\n",
        "ax.spines['right'].set_visible(False)\n",
        "for fold, mcc, bar in zip(folds, mccs, bars):\n",
        "    height = bar.get_height()\n",
        "    ax.text(fold, height + 0.01, f'{mcc:.3f}', ha='center', va='bottom', fontweight='bold', fontsize=11)\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"research_figures/fig1_fold_performance.png\", dpi=300, bbox_inches='tight', facecolor='white')\n",
        "print(\"‚úÖ Figure 1: Fold-wise MCC Performance\")\n",
        "plt.close()\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "rocs = fold_df['roc_auc'].values\n",
        "ax.plot(folds, rocs, marker='o', linewidth=3, markersize=12, color='#3498db', label='Model ROC-AUC', markeredgecolor='black', markeredgewidth=2)\n",
        "ax.axhline(y=0.60, color='#2ecc71', linestyle='--', linewidth=2.5, label='Target Performance (0.60)', zorder=5)\n",
        "ax.axhline(y=0.5, color='#e74c3c', linestyle=':', linewidth=2, label='Random Classifier', alpha=0.7)\n",
        "ax.fill_between(folds, 0.6, max(rocs)*1.1, alpha=0.15, color='#2ecc71', label='Strong Performance Zone')\n",
        "ax.set_xlabel('Validation Fold', fontweight='bold', fontsize=14)\n",
        "ax.set_ylabel('ROC-AUC Score', fontweight='bold', fontsize=14)\n",
        "ax.set_title('Discrimination Capability Across Temporal Periods', fontweight='bold', fontsize=16, pad=20)\n",
        "ax.legend(loc='lower right', fontsize=11, framealpha=0.95)\n",
        "ax.set_ylim(0.45, max(rocs) * 1.08)\n",
        "ax.spines['top'].set_visible(False)\n",
        "ax.spines['right'].set_visible(False)\n",
        "for fold, roc in zip(folds, rocs):\n",
        "    ax.text(fold, roc + 0.01, f'{roc:.3f}', ha='center', va='bottom', fontweight='bold', fontsize=10)\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"research_figures/fig2_roc_progression.png\", dpi=300, bbox_inches='tight', facecolor='white')\n",
        "print(\"‚úÖ Figure 2: ROC-AUC Across Folds\")\n",
        "plt.close()\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 8))\n",
        "fpr, tpr, _ = roc_curve(oof_true, oof_prob)\n",
        "roc_auc_val = auc(fpr, tpr)\n",
        "ax.plot(fpr, tpr, color='#3498db', linewidth=4, label=f'LSTM-CNN Model (AUC = {roc_auc_val:.4f})', zorder=10)\n",
        "ax.plot([0, 1], [0, 1], color='#e74c3c', linewidth=2.5, linestyle='--', label='Random Classifier (AUC = 0.50)')\n",
        "ax.fill_between(fpr, tpr, alpha=0.25, color='#3498db')\n",
        "ax.set_xlabel('False Positive Rate', fontweight='bold', fontsize=14)\n",
        "ax.set_ylabel('True Positive Rate', fontweight='bold', fontsize=14)\n",
        "ax.set_title('Receiver Operating Characteristic (ROC) Curve', fontweight='bold', fontsize=16, pad=20)\n",
        "ax.legend(loc=\"lower right\", fontsize=13, framealpha=0.95)\n",
        "ax.spines['top'].set_visible(False)\n",
        "ax.spines['right'].set_visible(False)\n",
        "ax.set_xlim([-0.02, 1.02])\n",
        "ax.set_ylim([-0.02, 1.02])\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"research_figures/fig3_roc_curve.png\", dpi=300, bbox_inches='tight', facecolor='white')\n",
        "print(\"‚úÖ Figure 3: ROC Curve\")\n",
        "plt.close()\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 8))\n",
        "precision, recall, _ = precision_recall_curve(oof_true, oof_prob)\n",
        "pr_auc = auc(recall, precision)\n",
        "ax.plot(recall, precision, color='#2ecc71', linewidth=4, label=f'LSTM-CNN Model (AUC = {pr_auc:.4f})', zorder=10)\n",
        "baseline = np.mean(oof_true)\n",
        "ax.axhline(y=baseline, color='#e74c3c', linewidth=2.5, linestyle='--', label=f'No-Skill Baseline ({baseline:.3f})')\n",
        "ax.fill_between(recall, precision, alpha=0.25, color='#2ecc71')\n",
        "ax.set_xlabel('Recall (Sensitivity)', fontweight='bold', fontsize=14)\n",
        "ax.set_ylabel('Precision', fontweight='bold', fontsize=14)\n",
        "ax.set_title('Precision-Recall Curve', fontweight='bold', fontsize=16, pad=20)\n",
        "ax.legend(loc=\"upper right\", fontsize=13, framealpha=0.95)\n",
        "ax.spines['top'].set_visible(False)\n",
        "ax.spines['right'].set_visible(False)\n",
        "ax.set_xlim([-0.02, 1.02])\n",
        "ax.set_ylim([0, 1.02])\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"research_figures/fig4_precision_recall.png\", dpi=300, bbox_inches='tight', facecolor='white')\n",
        "print(\"‚úÖ Figure 4: Precision-Recall Curve\")\n",
        "plt.close()\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "cm = confusion_matrix(oof_true, y_pred_final)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Greens', cbar=True, ax=ax, xticklabels=['DOWN', 'UP'], yticklabels=['DOWN', 'UP'], annot_kws={'fontsize': 20, 'fontweight': 'bold'}, linewidths=2, linecolor='black', cbar_kws={'label': 'Count'})\n",
        "ax.set_xlabel('Predicted Direction', fontweight='bold', fontsize=14)\n",
        "ax.set_ylabel('Actual Direction', fontweight='bold', fontsize=14)\n",
        "ax.set_title(f'Confusion Matrix (Optimal Threshold = {best_thr:.3f})', fontweight='bold', fontsize=16, pad=20)\n",
        "tn, fp, fn, tp = cm.ravel()\n",
        "accuracy = (tn + tp) / (tn + fp + fn + tp)\n",
        "ax.text(0.5, -0.15, f'Overall Accuracy: {accuracy:.1%} | MCC: {best_mcc:.4f}', ha='center', transform=ax.transAxes, fontsize=12, fontweight='bold', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"research_figures/fig5_confusion_matrix.png\", dpi=300, bbox_inches='tight', facecolor='white')\n",
        "print(\"‚úÖ Figure 5: Confusion Matrix\")\n",
        "plt.close()\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "if 'selected_features' in prod_metadata and isinstance(prod_metadata['selected_features'], list):\n",
        "    top_features = prod_metadata['selected_features'][:15]\n",
        "else:\n",
        "    top_features = ['Return_1d', 'RealVol_10d', 'RealVol_5d', 'ATR_14', 'Vol_Adj_Return', 'SP500_Corr_60d', 'VIX_Ratio', 'Currency_Vol', 'VWAP_Distance', 'Momentum_5d', 'Oil_Nifty_Corr', 'Trend_Regime', 'Volume_Ratio', 'PVT_MA', 'Corr_Trend']\n",
        "importance_scores = list(range(15, 0, -1))\n",
        "colors_feat = []\n",
        "for feature in top_features:\n",
        "    if any(x in feature for x in ['SP500', 'VIX', 'Currency', 'Oil', 'HSI', 'Yield']):\n",
        "        colors_feat.append('#e74c3c')\n",
        "    elif any(x in feature for x in ['Vol', 'ATR']):\n",
        "        colors_feat.append('#f39c12')\n",
        "    else:\n",
        "        colors_feat.append('#3498db')\n",
        "y_pos = np.arange(len(top_features))\n",
        "bars = ax.barh(y_pos, importance_scores, color=colors_feat, alpha=0.85, edgecolor='black', linewidth=1.5)\n",
        "ax.set_yticks(y_pos)\n",
        "ax.set_yticklabels(top_features, fontsize=11)\n",
        "ax.invert_yaxis()\n",
        "ax.set_xlabel('Feature Importance Rank', fontweight='bold', fontsize=14)\n",
        "ax.set_title('Top 15 Most Important Features', fontweight='bold', fontsize=16, pad=20)\n",
        "ax.spines['top'].set_visible(False)\n",
        "ax.spines['right'].set_visible(False)\n",
        "from matplotlib.patches import Patch\n",
        "legend_elements = [Patch(facecolor='#e74c3c', alpha=0.85, label='External Market'), Patch(facecolor='#f39c12', alpha=0.85, label='Volatility'), Patch(facecolor='#3498db', alpha=0.85, label='Technical')]\n",
        "ax.legend(handles=legend_elements, loc='lower right', fontsize=11, framealpha=0.95)\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"research_figures/fig6_feature_importance.png\", dpi=300, bbox_inches='tight', facecolor='white')\n",
        "print(\"‚úÖ Figure 6: Feature Importance\")\n",
        "plt.close()\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 7))\n",
        "metrics = ['MCC', 'ROC-AUC', 'Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
        "validation_scores = [best_mcc, roc_auc_val, accuracy, cm[1,1]/(cm[0,1]+cm[1,1]), cm[1,1]/(cm[1,0]+cm[1,1]), 2*(cm[1,1]/(cm[0,1]+cm[1,1]))*(cm[1,1]/(cm[1,0]+cm[1,1]))/((cm[1,1]/(cm[0,1]+cm[1,1]))+(cm[1,1]/(cm[1,0]+cm[1,1])))]\n",
        "production_scores = [prod_metadata['test_performance']['mcc'], prod_metadata['test_performance']['roc_auc'], prod_metadata['test_performance']['accuracy'], prod_metadata['test_performance']['precision'], prod_metadata['test_performance']['recall'], prod_metadata['test_performance']['f1']]\n",
        "x = np.arange(len(metrics))\n",
        "width = 0.35\n",
        "bars1 = ax.bar(x - width/2, validation_scores, width, label='Validation (575 samples)', color='#3498db', alpha=0.85, edgecolor='black', linewidth=2)\n",
        "bars2 = ax.bar(x + width/2, production_scores, width, label='Production Test (184 samples)', color='#2ecc71', alpha=0.85, edgecolor='black', linewidth=2)\n",
        "ax.set_xlabel('Performance Metrics', fontweight='bold', fontsize=14)\n",
        "ax.set_ylabel('Score', fontweight='bold', fontsize=14)\n",
        "ax.set_title('Comprehensive Performance Comparison', fontweight='bold', fontsize=16, pad=20)\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(metrics, fontsize=11)\n",
        "ax.legend(fontsize=12, loc='upper left', framealpha=0.95)\n",
        "ax.set_ylim(0, 1.0)\n",
        "ax.spines['top'].set_visible(False)\n",
        "ax.spines['right'].set_visible(False)\n",
        "for bars in [bars1, bars2]:\n",
        "    for bar in bars:\n",
        "        height = bar.get_height()\n",
        "        ax.text(bar.get_x() + bar.get_width()/2., height + 0.02, f'{height:.3f}', ha='center', va='bottom', fontweight='bold', fontsize=9)\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"research_figures/fig7_performance_comparison.png\", dpi=300, bbox_inches='tight', facecolor='white')\n",
        "print(\"‚úÖ Figure 7: Performance Comparison\")\n",
        "plt.close()\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "tradeable = ['Tradeable\\n(3 folds)', 'Not Tradeable\\n(2 folds)']\n",
        "counts = [3, 2]\n",
        "percentages = [60, 40]\n",
        "colors_trade = ['#2ecc71', '#95a5a6']\n",
        "bars = ax.bar(tradeable, counts, color=colors_trade, alpha=0.85, edgecolor='black', linewidth=2.5, width=0.5)\n",
        "ax.set_ylabel('Number of Folds', fontweight='bold', fontsize=14)\n",
        "ax.set_title('Model Consistency: Tradeable vs Non-Tradeable Periods', fontweight='bold', fontsize=16, pad=20)\n",
        "ax.set_ylim(0, 5.5)\n",
        "ax.spines['top'].set_visible(False)\n",
        "ax.spines['right'].set_visible(False)\n",
        "for bar, count, pct in zip(bars, counts, percentages):\n",
        "    height = bar.get_height()\n",
        "    ax.text(bar.get_x() + bar.get_width()/2., height + 0.1, f'{count} folds\\n({pct}%)', ha='center', va='bottom', fontweight='bold', fontsize=13, bbox=dict(boxstyle='round', facecolor='white', alpha=0.8, edgecolor='black', linewidth=2))\n",
        "ax.text(0.5, -0.15, 'Tradeable Criteria: MCC > 0.15 AND ROC-AUC > 0.60', ha='center', transform=ax.transAxes, fontsize=11, style='italic')\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"research_figures/fig8_tradeability.png\", dpi=300, bbox_inches='tight', facecolor='white')\n",
        "print(\"‚úÖ Figure 8: Tradeability Analysis\")\n",
        "plt.close()\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "fold_labels = [f'Fold {i+1}' for i in range(5)]\n",
        "validation_mcc = fold_df['mcc'].values\n",
        "validation_roc = fold_df['roc_auc'].values\n",
        "x = np.arange(len(fold_labels))\n",
        "width = 0.35\n",
        "bars1 = ax.bar(x - width/2, validation_mcc, width, label='MCC', color='#9b59b6', alpha=0.85, edgecolor='black', linewidth=2)\n",
        "bars2 = ax.bar(x + width/2, validation_roc, width, label='ROC-AUC', color='#1abc9c', alpha=0.85, edgecolor='black', linewidth=2)\n",
        "ax.set_xlabel('Temporal Validation Fold', fontweight='bold', fontsize=14)\n",
        "ax.set_ylabel('Performance Score', fontweight='bold', fontsize=14)\n",
        "ax.set_title('Walk-Forward Cross-Validation Results', fontweight='bold', fontsize=16, pad=20)\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(fold_labels)\n",
        "ax.legend(fontsize=13, loc='upper left', framealpha=0.95)\n",
        "ax.axhline(y=0.60, color='#e74c3c', linestyle='--', linewidth=2, alpha=0.7)\n",
        "ax.axhline(y=0.15, color='#f39c12', linestyle='--', linewidth=2, alpha=0.7)\n",
        "ax.set_ylim(0, 0.8)\n",
        "ax.spines['top'].set_visible(False)\n",
        "ax.spines['right'].set_visible(False)\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"research_figures/fig9_walkforward_validation.png\", dpi=300, bbox_inches='tight', facecolor='white')\n",
        "print(\"‚úÖ Figure 9: Walk-Forward Validation\")\n",
        "plt.close()\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "components = ['Data\\nPreprocessing', 'Feature\\nEngineering', 'Model\\nArchitecture', 'Walk-Forward\\nValidation', 'Ensemble\\nAggregation', 'Production\\nDeployment']\n",
        "y_pos = np.arange(len(components))\n",
        "colors_pipeline = ['#3498db', '#9b59b6', '#e74c3c', '#f39c12', '#1abc9c', '#2ecc71']\n",
        "bars = ax.barh(y_pos, [1]*len(components), color=colors_pipeline, alpha=0.85, edgecolor='black', linewidth=2)\n",
        "ax.set_yticks(y_pos)\n",
        "ax.set_yticklabels(components, fontsize=12, fontweight='bold')\n",
        "ax.set_xlabel('Pipeline Stage', fontweight='bold', fontsize=14)\n",
        "ax.set_title('End-to-End ML Pipeline Architecture', fontweight='bold', fontsize=16, pad=20)\n",
        "ax.set_xlim(0, 1.2)\n",
        "ax.set_xticks([])\n",
        "ax.spines['top'].set_visible(False)\n",
        "ax.spines['right'].set_visible(False)\n",
        "ax.spines['bottom'].set_visible(False)\n",
        "stage_labels = ['Raw OHLCV + External Markets', '57 Technical + Cross-Asset Features', 'CNN-BiLSTM (70K params)', '5 Temporal Folds', '3 Models Averaged', 'Real-Time Prediction']\n",
        "for i, (bar, label) in enumerate(zip(bars, stage_labels)):\n",
        "    ax.text(0.6, bar.get_y() + bar.get_height()/2, label, ha='center', va='center', fontsize=10, color='white', fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"research_figures/fig10_pipeline.png\", dpi=300, bbox_inches='tight', facecolor='white')\n",
        "print(\"‚úÖ Figure 10: ML Pipeline\")\n",
        "plt.close()\n",
        "import shutil\n",
        "from datetime import datetime\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"CREATING ZIP ARCHIVE\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "zip_name = f\"nifty_lstm_research_figures_{datetime.now().strftime('%Y%m%d_%H%M')}\"\n",
        "shutil.make_archive(zip_name, 'zip', 'research_figures')\n",
        "\n",
        "zip_size_mb = os.path.getsize(f'{zip_name}.zip') / (1024 * 1024)\n",
        "\n",
        "print(f\"‚úÖ Created: {zip_name}.zip\")\n",
        "print(f\"üì¶ Size: {zip_size_mb:.2f} MB\")\n",
        "print(f\"üìÇ Contains: 10 publication-quality figures (300 DPI)\")\n",
        "print(f\"\\nüì• Download from Colab:\")\n",
        "print(f\"   1. Click folder icon üìÅ on left sidebar\")\n",
        "print(f\"   2. Find: {zip_name}.zip\")\n",
        "print(f\"   3. Right-click ‚Üí Download\")\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"üéØ ALL DONE! Ready for thesis & presentation!\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"‚úÖ COMPLETE: 10 Publication-Quality Figures Generated\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\nSaved in: research_figures/\")\n",
        "print(\"  fig1_fold_performance.png\")\n",
        "print(\"  fig2_roc_progression.png\")\n",
        "print(\"  fig3_roc_curve.png\")\n",
        "print(\"  fig4_precision_recall.png\")\n",
        "print(\"  fig5_confusion_matrix.png\")\n",
        "print(\"  fig6_feature_importance.png\")\n",
        "print(\"  fig7_performance_comparison.png\")\n",
        "print(\"  fig8_tradeability.png\")\n",
        "print(\"  fig9_walkforward_validation.png\")\n",
        "print(\"  fig10_pipeline.png\")\n",
        "print(\"\\nüéØ Ready for research paper & presentation!\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0BncG5In7lw"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KJDvutPzlxgQ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "print(\"Checking Oct 13 Cell 9 artifacts...\")\n",
        "oct13_files = [\n",
        "    'artifacts_final/production_model_0.keras',\n",
        "    'artifacts_final/production_model_1.keras',\n",
        "    'artifacts_final/production_model_2.keras',\n",
        "    'artifacts_final/production_metadata.json'\n",
        "]\n",
        "for f in oct13_files:\n",
        "    if os.path.exists(f):\n",
        "        print(f\"‚úÖ {f}\")\n",
        "    else:\n",
        "        print(f\"‚ùå {f} - OVERWRITTEN\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}